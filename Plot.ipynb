{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f5389be",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc49b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Pyro diagnostics\n",
    "from pyro.ops.stats import gelman_rubin, split_gelman_rubin, effective_sample_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f5abc",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(csv_path, col_name = \"estimated_params\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    estimated_params_list = df[col_name].apply(ast.literal_eval).tolist()\n",
    "    return np.array(estimated_params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511aa85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Data/X_train.csv', header=None, delimiter=',').values\n",
    "X_test = pd.read_csv('Data/X_test.csv', header=None, delimiter=',').values\n",
    "\n",
    "Y_train_21 = pd.read_csv('Data/Y_train_std_21.csv', header=None, delimiter=',').values\n",
    "Y_test_21 = pd.read_csv('Data/Y_test_std_21.csv', header=None, delimiter=',').values\n",
    "\n",
    "Y_train_std = pd.read_csv('Data/Y_train_std.csv', header=None, delimiter=',').values\n",
    "Y_test_std = pd.read_csv('Data/Y_test_std.csv', header=None, delimiter=',').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60354282",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_GP_params = extract_data(\"PlotData/L.GP_21_result.csv\")\n",
    "\n",
    "L_DKGP_params = extract_data(\"PlotData/L.DKGP_21_result.csv\")\n",
    "\n",
    "L_MGP_params = extract_data(\"PlotData/L.MGP_21_result.csv\")\n",
    "\n",
    "L_DKMGP_params = extract_data(\"PlotData/L.DKMGP_21_result.csv\")\n",
    "\n",
    "VGP_params = extract_data(\"PlotData/VGP_21_result.csv\")\n",
    "\n",
    "MVGP_params = extract_data(\"PlotData/MVGP_21_result.csv\")\n",
    "\n",
    "DGP_params = extract_data(\"PlotData/DGP_21_point_result.csv\")\n",
    "\n",
    "DNN_params = extract_data(\"PlotData/DNN_21_result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee0227",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_specific_format_data(folder, n_samples):\n",
    "    \"\"\"\n",
    "    专门读取 {'param_0': tensor, ...} 格式的数据\n",
    "    返回形状: (100, 1200, 10) -> (样本ID, 后验采样ID, 参数维度)\n",
    "    \"\"\"\n",
    "    # 预分配内存\n",
    "    all_posteriors = np.zeros((n_samples, N_POSTERIOR, N_DIMS))\n",
    "    \n",
    "    print(f\"{n_samples}...\")\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        file_path = os.path.join(folder, f'result_{i+1}.pkl')\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"{file_path}did not exist.\")\n",
    "            continue\n",
    "            \n",
    "        with open(file_path, 'rb') as f:\n",
    "            # data 是一个字典: {'param_0': tensor([...]), ...}\n",
    "            data_dict = pickle.load(f)\n",
    "        \n",
    "        # 将字典转换为 (1200, 10) 的 numpy 数组\n",
    "        # 我们必须显式循环 0-9，以确保参数顺序正确\n",
    "        sample_matrix = []\n",
    "        for dim in range(N_DIMS):\n",
    "            key = f'param_{dim}'\n",
    "            if key not in data_dict:\n",
    "                raise ValueError(f\"{file_path} lack {key}\")\n",
    "            \n",
    "            # 提取 tensor 并转为 numpy\n",
    "            # .detach().cpu().numpy() 是处理 tensor 的通用安全写法\n",
    "            tensor_data = data_dict[key]\n",
    "            numpy_data = tensor_data.detach().cpu().numpy()\n",
    "            \n",
    "            sample_matrix.append(numpy_data)\n",
    "        \n",
    "        # sample_matrix 目前是 list of 10 arrays (each 1200 length)\n",
    "        # 使用 column_stack 变成 (1200, 10)\n",
    "        sample_matrix = np.column_stack(sample_matrix)\n",
    "        \n",
    "        # 存入大数组\n",
    "        all_posteriors[i, :, :] = sample_matrix\n",
    "\n",
    "    print(\"Shape:\", all_posteriors.shape)\n",
    "    return all_posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ed8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_estimates(samples):\n",
    "    \"\"\"\n",
    "    输入: samples (1200,) 一维数组\n",
    "    输出: mean, median, map_estimate\n",
    "    \"\"\"\n",
    "    # 1. Mean\n",
    "    mu = np.mean(samples)\n",
    "    \n",
    "    # 2. Median (50th percentile)\n",
    "    med = np.median(samples)\n",
    "    \n",
    "    # 3. MAP (Mode) - 通过 KDE 寻找峰值\n",
    "    try:\n",
    "        # 使用高斯核密度估计拟合分布\n",
    "        kde = gaussian_kde(samples)\n",
    "        # 生成密集的 x 网格\n",
    "        x_grid = np.linspace(np.min(samples), np.max(samples), 500)\n",
    "        # 找到密度最大的 x\n",
    "        density = kde(x_grid)\n",
    "        map_est = x_grid[np.argmax(density)]\n",
    "    except:\n",
    "        # 如果样本方差为0等极端情况，退化为中位数\n",
    "        map_est = med\n",
    "        \n",
    "    return mu, med, map_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2074e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chain(chain_tensor: torch.Tensor):\n",
    "    \"\"\"\n",
    "    将单链样本拆成两半，形成两条“伪链”\n",
    "    输入: Tensor[N]\n",
    "    输出: (Tensor[N//2], Tensor[N//2])\n",
    "    \"\"\"\n",
    "    n = chain_tensor.shape[0]\n",
    "    half = n // 2\n",
    "    return chain_tensor[:half], chain_tensor[half:2*half]\n",
    "\n",
    "\n",
    "def visualize_posterior_1d_params(\n",
    "    single_chain_samples: dict,\n",
    "    *,\n",
    "    true_params_tensor=None,        # <- 新增：torch Tensor[D]，比如 test_x[0]\n",
    "    bins=15,\n",
    "    acf_lags=40,\n",
    "    clip_percentiles=(0.5, 99.5),\n",
    "    xlim=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    single_chain_samples: dict[name -> Tensor[N]]\n",
    "    true_params_tensor: Tensor[D]，若提供，会在 trace/hist 上标出真实值\n",
    "    对每个参数：split Rhat/ESS + trace + hist+KDE+quantiles + ACF\n",
    "    \"\"\"\n",
    "\n",
    "    # --- display name mapping (theta_0..theta_9 -> Ca_1..Cb_5) ---\n",
    "    param_labels = [\"Ca_1\",\"Cb_1\",\"Ca_2\",\"Cb_2\",\"Ca_3\",\"Cb_3\",\"Ca_4\",\"Cb_4\",\"Ca_5\",\"Cb_5\"]\n",
    "    name_map = {f\"theta_{i}\": param_labels[i] for i in range(len(param_labels))}\n",
    "    name_map.update({f\"param_{i}\": param_labels[i] for i in range(len(param_labels))})\n",
    "\n",
    "    def display_name(p: str) -> str:\n",
    "        return name_map.get(p, p)\n",
    "\n",
    "    def parse_index(p: str):\n",
    "        # 只对 theta_7 / param_7 这种形式返回 index\n",
    "        try:\n",
    "            head, tail = p.split(\"_\", 1)\n",
    "            if head in (\"theta\", \"param\"):\n",
    "                return int(tail)\n",
    "        except Exception:\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "    # 真实参数向量准备\n",
    "    if true_params_tensor is not None:\n",
    "        if not torch.is_tensor(true_params_tensor):\n",
    "            true_vec = torch.tensor(true_params_tensor, dtype=torch.float32).detach().flatten().cpu()\n",
    "    else:\n",
    "        true_vec = None\n",
    "\n",
    "    # 整理成 mcmc_samples：param -> Tensor[2, n_half]\n",
    "    mcmc_samples = {}\n",
    "    for param, samples in single_chain_samples.items():\n",
    "        if samples.ndim != 1:\n",
    "            raise ValueError(f\"{param} should be 1D Tensor[N], got shape {tuple(samples.shape)}\")\n",
    "        chain_a, chain_b = split_chain(samples)\n",
    "        mcmc_samples[param] = torch.stack([chain_a, chain_b], dim=0)  # [2, n_half]\n",
    "\n",
    "    # 诊断和可视化\n",
    "    for param, samples_chains in mcmc_samples.items():\n",
    "        disp = display_name(param)\n",
    "\n",
    "        idx = parse_index(param)\n",
    "        true_val = None\n",
    "        if (true_vec is not None) and (idx is not None) and (0 <= idx < true_vec.numel()):\n",
    "            true_val = float(true_vec[idx].item())\n",
    "\n",
    "        rhat = gelman_rubin(samples_chains, chain_dim=0, sample_dim=1)\n",
    "        split_rhat = split_gelman_rubin(samples_chains, chain_dim=0, sample_dim=1)\n",
    "        ess = effective_sample_size(samples_chains, chain_dim=0, sample_dim=1)\n",
    "\n",
    "        if true_val is None:\n",
    "            print(f\"{disp}: R-hat = {rhat:.3f}, split R-hat = {split_rhat:.3f}, ESS = {ess:.1f}\")\n",
    "        else:\n",
    "            print(f\"{disp}: R-hat = {rhat:.3f}, split R-hat = {split_rhat:.3f}, ESS = {ess:.1f}, true = {true_val:.6g}\")\n",
    "\n",
    "        # --- Trace + Histogram/KDE ---\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        # Trace\n",
    "        plt.subplot(1, 2, 1)\n",
    "        for i in range(2):\n",
    "            plt.plot(samples_chains[i].cpu().numpy(), marker='o', label=f\"Chain {i+1}\", alpha=0.7)\n",
    "\n",
    "        if true_val is not None:\n",
    "            plt.axhline(true_val, linestyle=\"--\", linewidth=2, label=\"True value\", color=\"green\")  # 真实值水平线\n",
    "\n",
    "        plt.title(f\"Trace Plot for {disp}\")\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.ylabel(disp)\n",
    "        plt.legend()\n",
    "\n",
    "        # Histogram + KDE + Quantiles\n",
    "        plt.subplot(1, 2, 2)\n",
    "        all_samps = samples_chains.reshape(-1).cpu().numpy()\n",
    "\n",
    "        p_lo, p_hi = clip_percentiles\n",
    "        xmin, xmax = np.percentile(all_samps, [p_lo, p_hi])\n",
    "\n",
    "        # 确保真实值在线性范围内（避免被裁掉看不到）\n",
    "        if true_val is not None:\n",
    "            xmin = min(xmin, true_val)\n",
    "            xmax = max(xmax, true_val)\n",
    "            # 给一点边距\n",
    "            pad = 0.02 * (xmax - xmin + 1e-12)\n",
    "            xmin, xmax = xmin - pad, xmax + pad\n",
    "\n",
    "        plt.hist(all_samps, bins=bins, density=True, alpha=0.7, color='gray')\n",
    "\n",
    "        # KDE（样本太少/方差太小有时会报错，所以做个保护）\n",
    "        if np.std(all_samps) > 0 and len(all_samps) > 5:\n",
    "            kde = gaussian_kde(all_samps)\n",
    "            x_grid = np.linspace(xmin, xmax, 200)\n",
    "            plt.plot(x_grid, kde(x_grid), linewidth=2)\n",
    "\n",
    "        qs = torch.quantile(torch.from_numpy(all_samps), torch.tensor([0.025, 0.5, 0.975]))\n",
    "        for q in qs:\n",
    "            plt.axvline(q.item(), color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "        if true_val is not None:\n",
    "            plt.axvline(true_val, linewidth=2, label=\"True value\", color=\"green\")  # 真实值竖线\n",
    "\n",
    "        if xlim is not None:\n",
    "            plt.xlim(*xlim)\n",
    "        else:\n",
    "            plt.xlim(xmin, xmax)\n",
    "\n",
    "        plt.title(f\"Histogram + 2.5/50/97.5% Quantiles ({disp})\")\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # --- ACF（仅第一“伪链”） ---\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plot_acf(samples_chains[0].cpu().numpy(), lags=acf_lags)\n",
    "        plt.title(f\"ACF for {disp} (Chain 1)\")\n",
    "        plt.xlabel(\"Lag\")\n",
    "        plt.ylabel(\"Autocorrelation\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e051d9",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84027685",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"PlotData\"\n",
    "\n",
    "model_files = {\n",
    "    \"L_GP\": \"L.GP_21_result.csv\",\n",
    "    \"L_MGP\": \"L.MGP_21_result.csv\",\n",
    "    \"L_DKGP\": \"L.DKGP_21_result.csv\", \n",
    "    \"L_DKMGP\": \"L.DKMGP_21_result.csv\",\n",
    "    \"VGP\": \"VGP_21_result.csv\",\n",
    "    \"MVGP\": \"MVGP_21_result.csv\",\n",
    "    \"DGP\": \"DGP_21_point_result.csv\",\n",
    "    \"DNN\": \"DNN_21_result.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545e2c4",
   "metadata": {},
   "source": [
    "## pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c672b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_results = {}\n",
    "for model, filename in model_files.items():\n",
    "    est = extract_data(os.path.join(base_dir, filename), col_name = 'test_preds')\n",
    "\n",
    "    mse_results[model] = np.mean((est - Y_test_21) ** 2)\n",
    "\n",
    "\n",
    "\n",
    "mse_df = pd.DataFrame.from_dict(mse_results, orient='index', columns=['MSE'])\n",
    "mse_df.index.name = 'Model'\n",
    "mse_df = mse_df.reset_index()\n",
    "\n",
    "\n",
    "print(mse_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b62e3",
   "metadata": {},
   "source": [
    "## Single vs. Multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2604b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_dict = {}\n",
    "for model, filename in model_files.items():\n",
    "    est = extract_data(os.path.join(base_dir, filename))\n",
    "\n",
    "    mse_per_row = np.mean((est - X_test) ** 2, axis=1)\n",
    "    mse_dict[model] = mse_per_row\n",
    "\n",
    "\n",
    "\n",
    "mse_df = pd.DataFrame(mse_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792fac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed706ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([mse_df[model] for model in mse_df.columns], labels=mse_df.columns)\n",
    "plt.ylabel(\"Mean Squared Error per Sample\")\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "# plt.ylim(0, 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20acf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.boxplot([mse_df[model] for model in mse_df.columns], labels=mse_df.columns)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel(\"MSE per Sample (log scale)\")\n",
    "ax.set_title(\"Comparison with Log-scaled\")\n",
    "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f40070",
   "metadata": {},
   "source": [
    "## Local vs. Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_FOLDER = 'PlotData/L.DKMGP_21_mcmc_result'    # 1500\n",
    "DATA_FOLDER = 'PlotData/MVGP_21_mcmc_result'       # 1200\n",
    "# DATA_FOLDER = 'PlotData/DNN_21_mcmc_result'          # 1200\n",
    "N_SAMPLES = 120\n",
    "N_POSTERIOR = 1200\n",
    "N_DIMS = 10\n",
    "\n",
    "param_labels = [\"Ca_1\",\"Cb_1\",\"Ca_2\",\"Cb_2\",\"Ca_3\",\"Cb_3\",\"Ca_4\",\"Cb_4\",\"Ca_5\",\"Cb_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = load_specific_format_data(DATA_FOLDER, N_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e758c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_params = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = posterior_samples.shape[0]\n",
    "N_DIMS = posterior_samples.shape[2]\n",
    "\n",
    "\n",
    "est_mean = np.zeros((N_SAMPLES, N_DIMS))\n",
    "est_median = np.zeros((N_SAMPLES, N_DIMS))\n",
    "est_map = np.zeros((N_SAMPLES, N_DIMS))\n",
    "\n",
    "print(\"Mean, Median, MAP\")\n",
    "for i in range(N_SAMPLES):\n",
    "    for d in range(N_DIMS):\n",
    "        s = posterior_samples[i, :, d]\n",
    "        mu, med, mode = get_point_estimates(s)\n",
    "        \n",
    "        est_mean[i, d] = mu\n",
    "        est_median[i, d] = med\n",
    "        est_map[i, d] = mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_mean = est_map # MAP 作为最终点估计\n",
    "# post_mean = np.mean(posterior_samples, axis=1) # 后验均值\n",
    "lower_ci = np.percentile(posterior_samples, 2.5, axis=1)  # 95% HDI 下界\n",
    "upper_ci = np.percentile(posterior_samples, 97.5, axis=1) # 95% HDI 上界\n",
    "\n",
    "# A. RMSE (均方根误差)\n",
    "rmse = np.sqrt(np.mean((post_mean - true_params)**2, axis=0))\n",
    "\n",
    "# B. Bias (偏差)\n",
    "bias = np.mean(post_mean - true_params, axis=0)\n",
    "\n",
    "# C. Coverage Probability (覆盖率)\n",
    "# 检查真实值是否在 95% 区间内\n",
    "is_covered = (true_params >= lower_ci) & (true_params <= upper_ci)\n",
    "coverage = np.mean(is_covered, axis=0)\n",
    "\n",
    "# 打印文本报告\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"{'Dim':<5} | {'RMSE':<10} | {'Bias':<10} | {'Coverage (Target: 0.95)':<25}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(N_DIMS):\n",
    "    flag = \"\"\n",
    "    if coverage[i] < 0.90: flag = \"⚠️ Under\"\n",
    "    if coverage[i] > 0.99: flag = \"⚠️ Over\"\n",
    "    print(f\"{i:<5} | {rmse[i]:<10.4f} | {bias[i]:<10.4f} | {coverage[i]:<6.2f} {flag}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. 可视化\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig1, ax1 = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "ax1.scatter(true_params.flatten(), post_mean.flatten(),\n",
    "            alpha=0.3, s=10, c='royalblue')\n",
    "\n",
    "min_val = min(true_params.min(), post_mean.min())\n",
    "max_val = max(true_params.max(), post_mean.max())\n",
    "ax1.plot([min_val, max_val], [min_val, max_val],\n",
    "         'r--', lw=2, label='Perfect Fit')\n",
    "\n",
    "ax1.set_xlabel('True Parameter Value')\n",
    "ax1.set_ylabel('Posterior Mean Estimate')\n",
    "ax1.set_title(f'Global Accuracy (All Params)\\nOverall RMSE: {np.mean(rmse):.4f}')\n",
    "ax1.legend()\n",
    "fig1.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 图 2: Coverage Bar Plot\n",
    "# -------------------------\n",
    "fig2, ax2 = plt.subplots(figsize=(9, 4.8))\n",
    "\n",
    "x_pos = np.arange(N_DIMS)\n",
    "bars = ax2.bar(x_pos, coverage, color='#69b3a2', alpha=0.8, edgecolor='black')\n",
    "ax2.axhline(0.95, color='r', linestyle='--', linewidth=2, label='Target (0.95)')\n",
    "\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(param_labels, rotation=45, ha=\"right\")\n",
    "ax2.set_xlabel(\"Parameter\")\n",
    "ax2.set_ylabel('Coverage Probability')\n",
    "ax2.set_title('Uncertainty Calibration Check')\n",
    "\n",
    "for i, bar in enumerate(bars):\n",
    "    if coverage[i] < 0.9:\n",
    "        bar.set_color('#d62728')\n",
    "        bar.set_alpha(0.6)\n",
    "\n",
    "ax2.legend()\n",
    "fig2.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 图 3: Bias Boxplot\n",
    "# -------------------------\n",
    "fig3, ax3 = plt.subplots(figsize=(9, 4.8))\n",
    "\n",
    "bias_distribution = post_mean - true_params\n",
    "sns.boxplot(data=bias_distribution, ax=ax3, orient='h', palette=\"Set2\")\n",
    "\n",
    "ax3.axvline(0, color='r', linestyle='--')\n",
    "ax3.set_yticks(np.arange(len(param_labels)))\n",
    "ax3.set_yticklabels(param_labels)\n",
    "ax3.set_ylabel(\"Parameter\")\n",
    "ax3.set_xlabel('Bias: (MAP - True)')\n",
    "ax3.set_title('Bias Distribution (MAP Check)')\n",
    "fig3.tight_layout()\n",
    "# ax3.set_xlim(-0.5, 0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 图 4: Recovery Plot for Param_0 (or target_dim)\n",
    "# -------------------------\n",
    "fig4, ax4 = plt.subplots(figsize=(12, 5.5))\n",
    "\n",
    "target_dim = 0\n",
    "sort_indices = np.argsort(true_params[:, target_dim])\n",
    "\n",
    "sorted_true  = true_params[sort_indices, target_dim]\n",
    "sorted_mean  = post_mean[sort_indices, target_dim]\n",
    "sorted_lower = lower_ci[sort_indices, target_dim]\n",
    "sorted_upper = upper_ci[sort_indices, target_dim]\n",
    "\n",
    "x_axis = np.arange(N_SAMPLES)\n",
    "ax4.plot(x_axis, sorted_true, 'r-', linewidth=2, label='True Value')\n",
    "ax4.fill_between(x_axis, sorted_lower, sorted_upper, color='gray', alpha=0.3, label='95% Credible Interval')\n",
    "ax4.scatter(x_axis, sorted_mean, s=15, color='black', alpha=0.7, label='Posterior Mean')\n",
    "\n",
    "ax4.set_title(f'Detailed Recovery Check for Param_{target_dim} (Sorted by True Value)')\n",
    "ax4.set_xlabel('Sample Index')\n",
    "ax4.set_ylabel('Value')\n",
    "ax4.legend()\n",
    "\n",
    "fig4.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_crps_energy(samples, truth):\n",
    "    \"\"\"\n",
    "    基于 Energy Score 公式计算一组样本的 CRPS\n",
    "    输入:\n",
    "        samples: (S,) 1D array, 后验采样 (例如 1200 个)\n",
    "        truth: float, 真实值\n",
    "    输出:\n",
    "        crps: float\n",
    "    \"\"\"\n",
    "    # 1. 第一项: 绝对误差的期望 E[|X - y|]\n",
    "    term1 = np.mean(np.abs(samples - truth))\n",
    "    \n",
    "    # 2. 第二项: 样本间差异的期望 E[|X - X'|]\n",
    "    # 为了加速计算，我们使用排序后的技巧 (Hersbach decomposition) \n",
    "    # 或者直接广播 (S < 2000 时广播很快)\n",
    "    \n",
    "    # 方法 A: 广播法 (代码简单，S=1200 时内存占用约 11MB，完全可以接受)\n",
    "    # samples[:, None] - samples[None, :] 生成 (1200, 1200) 的矩阵\n",
    "    term2 = np.mean(np.abs(samples[:, None] - samples[None, :]))\n",
    "    \n",
    "    return term1 - 0.5 * term2\n",
    "\n",
    "# ==========================================\n",
    "# 批量计算 CRPS\n",
    "# ==========================================\n",
    "# 假设 posterior_samples 形状为 (100, 1200, 10)\n",
    "# 假设 true_params 形状为 (100, 10)\n",
    "\n",
    "N_SAMPLES = posterior_samples.shape[0] # 100\n",
    "N_DIMS = posterior_samples.shape[2]    # 10\n",
    "\n",
    "crps_results = np.zeros((N_SAMPLES, N_DIMS))\n",
    "\n",
    "print(\"开始计算 CRPS (这可能需要几秒钟)...\")\n",
    "for i in range(N_SAMPLES):\n",
    "    for d in range(N_DIMS):\n",
    "        s = posterior_samples[i, :, d] # 取出 1200 个样本\n",
    "        t = true_params[i, d]          # 取出 1 个真实值\n",
    "        crps_results[i, d] = compute_crps_energy(s, t)\n",
    "\n",
    "print(\"CRPS 计算完成。\")\n",
    "\n",
    "# 计算每个维度的平均 CRPS\n",
    "mean_crps_per_dim = np.mean(crps_results, axis=0)\n",
    "\n",
    "# 打印结果\n",
    "print(\"\\n=== CRPS Summary (Lower is Better) ===\")\n",
    "print(f\"{'Dim':<5} | {'Mean CRPS':<10}\")\n",
    "print(\"-\" * 20)\n",
    "for d in range(N_DIMS):\n",
    "    print(f\"{d:<5} | {mean_crps_per_dim[d]:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426daedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# --- 子图 1: 各维度的 CRPS 箱线图 ---\n",
    "fig1, ax1 = plt.subplots(figsize=(12, 5.5))\n",
    "sns.boxplot(data=crps_results, ax=ax1, palette=\"viridis\")\n",
    "ax1.set_xlabel('Parameter Dimension')\n",
    "ax1.set_xticklabels(param_labels, rotation=45, ha=\"right\")\n",
    "ax1.set_ylabel('Continuous Ranked Probability Score (CRPS)')\n",
    "ax1.set_title('CRPS Distribution across Dimensions')\n",
    "# 标出平均值\n",
    "ax1.plot(range(N_DIMS), mean_crps_per_dim, 'r--', marker='o', label='Mean CRPS')\n",
    "ax1.legend()\n",
    "\n",
    "# ax1.set_ylim(0, 0.5) \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(12, 5.5))\n",
    "posterior_std = np.std(posterior_samples, axis=1) # (100, 10)\n",
    "\n",
    "# 展平以便画散点图\n",
    "x_std = posterior_std.flatten()\n",
    "y_crps = crps_results.flatten()\n",
    "# 我们可以根据维度上色，看看是否有特定维度表现异常\n",
    "dim_labels = np.repeat(np.arange(N_DIMS), N_SAMPLES)\n",
    "\n",
    "scatter = ax2.scatter(x_std, y_crps, c=dim_labels, cmap='tab10', alpha=0.6, s=20)\n",
    "ax2.set_xlabel('Posterior Standard Deviation (Model Uncertainty)')\n",
    "ax2.set_ylabel('CRPS (Model Error)')\n",
    "ax2.set_title('Uncertainty vs. Performance Check')\n",
    "plt.colorbar(scatter, label='Dimension Index', ax=ax2)\n",
    "\n",
    "# 添加对角线参考 (仅作为视觉引导，非严格数学边界)\n",
    "# 如果点都在左上角 (低方差，高CRPS)，说明模型过度自信且错误\n",
    "ax2.text(0.05, 0.9, 'Danger Zone:\\nConfident but Wrong', transform=ax2.transAxes, \n",
    "         color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "fig2.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8fdf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_abs_sum = np.abs(bias_distribution).sum(axis=1)\n",
    "np.argsort(-row_abs_sum)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eea6b8",
   "metadata": {},
   "source": [
    "### mcmc plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbf4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"PlotData/DNN_21_mcmc_result/result_2.pkl\", \"rb\") as f:\n",
    "    loaded_samples_chain1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145495aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"PlotData/L.DKMGP_21_mcmc_result/result_2.pkl\", \"rb\") as f:\n",
    "    loaded_samples_chain1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"PlotData/MVGP_21_mcmc_result/result_22.pkl\", \"rb\") as f:\n",
    "    loaded_samples_chain1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7347c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GP_functions.Tools as Tools\n",
    "\n",
    "loaded_samples = torch.load(\"mcmc_RealCase_final_mcmc_infer_2block_independent.pt\", map_location='cpu')\n",
    "\n",
    "posterior_samples = Tools.extract_vector_params_from_mcmc(\n",
    "    loaded_samples,\n",
    "    key=\"params\",\n",
    "    param_names=[f\"theta_{i}\" for i in range(10)]  # 你也可以用真实参数名\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec29c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_posterior_1d_params(\n",
    "    loaded_samples_chain1,\n",
    "    true_params_tensor=X_test[21],\n",
    "    # true_params_tensor=None,\n",
    "    bins=15,\n",
    "    acf_lags=40,\n",
    "    clip_percentiles=(0.5, 99.5),\n",
    "    xlim=None, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee8ca7",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(\"PlotData\")\n",
    "\n",
    "\n",
    "PKL_PATTERN = re.compile(r\"result_(\\d+)\\.pkl\", re.IGNORECASE)\n",
    "\n",
    "true_params = X_test\n",
    "\n",
    "have_true = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001cfc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows: list[dict[str, float]] = []\n",
    "error_records: list[pd.DataFrame] = []  # for per‑sample error distributions\n",
    "\n",
    "for model_dir in sorted(ROOT.iterdir()):\n",
    "    if not model_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    model_name = model_dir.name\n",
    "    print(f\"Scanning {model_name} …\")\n",
    "\n",
    "    pkl_files = sorted(\n",
    "        (p for p in model_dir.glob(\"result_*.pkl\") if PKL_PATTERN.match(p.name)),\n",
    "        key=lambda p: int(PKL_PATTERN.search(p.name).group(1))\n",
    "    )\n",
    "\n",
    "    for pkl_path in pkl_files:\n",
    "        run_id = int(PKL_PATTERN.search(pkl_path.name).group(1))\n",
    "\n",
    "        with pkl_path.open(\"rb\") as fh:\n",
    "            posterior_dict = pickle.load(fh)\n",
    "\n",
    "        # (n_draws, 10)\n",
    "        samples = np.column_stack([\n",
    "            np.asarray(posterior_dict[f\"param_{i}\"], dtype=float).ravel()\n",
    "            for i in range(10)\n",
    "        ])\n",
    "\n",
    "        theta_star = true_params[run_id - 1]\n",
    "\n",
    "        # ── 2.1  Per‑run metrics ──────────────────────────────────────────\n",
    "        post_mean = samples.mean(axis=0)\n",
    "        rmse = float(np.sqrt(((post_mean - theta_star) ** 2).mean())) if have_true else np.nan\n",
    "\n",
    "        lo, hi = np.percentile(samples, [2.5, 97.5], axis=0)\n",
    "        cover95 = float(((theta_star >= lo) & (theta_star <= hi)).mean()) if have_true else np.nan\n",
    "\n",
    "        mean_post_sd = float(samples.std(axis=0).mean())\n",
    "\n",
    "        rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"run\": run_id,\n",
    "            \"rmse\": rmse,\n",
    "            \"cover95\": cover95,\n",
    "            \"post_sd\": mean_post_sd,\n",
    "            **{f\"phi{i+1}\": float(theta_star[i]) for i in range(10)},\n",
    "        })\n",
    "\n",
    "        # ── 2.2  Collect per‑sample error for NEW violin plot ─────────────\n",
    "        if have_true:\n",
    "            err = samples - theta_star  # shape (n_draws,10)\n",
    "            df_err = pd.DataFrame(err, columns=[f\"phi{i+1}\" for i in range(10)])\n",
    "            df_err = df_err.melt(var_name=\"param\", value_name=\"error\")\n",
    "            df_err.insert(0, \"model\", model_name)\n",
    "            error_records.append(df_err)\n",
    "\n",
    "print(f\"\\nLoaded {len(rows)} runs across {len(rows)//120 if rows else 0} models.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3  DATAFRAME & CSV OUTPUT\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values([\"model\", \"run\"])\n",
    "(df if have_true else df.drop(columns=[\"rmse\", \"cover95\"]))\\\n",
    "    .to_csv(ROOT / \"all_runs.csv\", index=False)\n",
    "print(\"Data table written → all_runs.csv\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4  VISUALISATIONS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "\n",
    "if have_true:\n",
    "    # 4.1  RMSE distribution per model\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(data=df, x=\"model\", y=\"rmse\", palette=\"Set2\", showfliers=False)\n",
    "    sns.stripplot(data=df, x=\"model\", y=\"rmse\", color=\"k\", size=3, alpha=0.4)\n",
    "    plt.title(\"RMSE distribution per model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ROOT / \"rmse_box.png\", dpi=150)\n",
    "\n",
    "# # 4.2  Parallel‑coordinates (parameters + scaled RMSE if available)\n",
    "# sc_df = df.copy()\n",
    "# scale_cols = [f\"phi{i}\" for i in range(1, 11)] + ([\"rmse\"] if have_true else [])\n",
    "# for c in scale_cols:\n",
    "#     span = sc_df[c].max() - sc_df[c].min()\n",
    "#     sc_df[c] = (sc_df[c] - sc_df[c].min()) / (span + 1e-12)\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# parallel_coordinates(\n",
    "#     sc_df[[\"model\", *scale_cols]],\n",
    "#     class_column=\"model\", alpha=0.3, linewidth=0.8\n",
    "# )\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.title(\"Parallel‑coordinates: 10‑D true parameters\" + (\" + scaled RMSE\" if have_true else \"\"))\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(ROOT / \"parallel_coords.png\", dpi=150)\n",
    "\n",
    "# # 4.3  PCA scatter (point size ∝ RMSE when available)\n",
    "# feat_cols = [f\"phi{i}\" for i in range(1, 11)]\n",
    "# pca = PCA(n_components=2).fit(df[feat_cols])\n",
    "# df[\"PC1\"], df[\"PC2\"] = pca.transform(df[feat_cols]).T\n",
    "# size = 40 + 260 * (df[\"rmse\"] - df[\"rmse\"].min()) / (df[\"rmse\"].max() - df[\"rmse\"].min() + 1e-12) if have_true else 60\n",
    "\n",
    "# plt.figure(figsize=(6, 5))\n",
    "# sns.scatterplot(\n",
    "#     data=df, x=\"PC1\", y=\"PC2\", hue=\"model\", size=size,\n",
    "#     palette=\"Set2\", sizes=(40, 300) if have_true else None,\n",
    "#     alpha=0.8, edgecolor=\"k\", legend=\"brief\"\n",
    "# )\n",
    "# plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)\")\n",
    "# plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)\")\n",
    "# plt.title(\"PCA of true parameters\" + (\"\\n(point size ∝ RMSE)\" if have_true else \"\"))\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(ROOT / \"pca_scatter.png\", dpi=150)\n",
    "\n",
    "# 4.4  NEW: Violin plot of posterior‑sample errors per parameter × model\n",
    "if have_true and error_records:\n",
    "    err_df = pd.concat(error_records, ignore_index=True)\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.violinplot(data=err_df, x=\"param\", y=\"error\", hue=\"model\",\n",
    "                   palette=\"Set2\", split=False, cut=0, inner=\"quartile\")\n",
    "    plt.axhline(0, color=\"k\", lw=.8, ls=\"--\")\n",
    "    plt.ylabel(\"Posterior sample − true value\")\n",
    "    plt.title(\"Distribution of posterior‑sample errors\\n(10 parameters × 3 models)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ROOT / \"error_violin.png\", dpi=150)\n",
    "\n",
    "print(\"Figures saved: rmse_box.png, parallel_coords.png, pca_scatter.png\" + (\", error_violin.png\" if have_true else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06419d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ef6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c38b2",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPTG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
