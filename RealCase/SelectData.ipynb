{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f07bee0",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14694ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11738394",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4c5ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data_train = pd.read_csv('../Data/Data20260110/Y_data_train_Final.csv', header=None, delimiter=',')\n",
    "# Y_data_train = Y_data_train.drop(columns=[Y_data_train.columns[-1]])\n",
    "\n",
    "Y_data_test = pd.read_csv('../Data/simulation_results_test.txt', header=None, delimiter=' ')\n",
    "Y_data_test = Y_data_test.drop(columns=[Y_data_test.columns[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4374fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data_max = pd.read_csv('simulation_results_5_max.txt', header=None, delimiter=' ')\n",
    "Y_data_max = Y_data_max.drop(columns=[Y_data_max.columns[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ac468",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data_extra = pd.read_csv('../simulation_results_Extra.txt', header=None, delimiter=' ')\n",
    "Y_data_extra = Y_data_extra.drop(columns=[Y_data_extra.columns[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41dc73b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RealCase = pd.read_csv('RealCase.csv', header=None, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1bfd3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = Y_data_train.drop(Y_data_train.columns[-18:], axis=1)\n",
    "Y_data_train = df_new.drop(df_new.columns[17], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e23baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = Y_data_test.drop(Y_data_test.columns[-18:], axis=1)\n",
    "Y_data_test = df_new.drop(df_new.columns[17], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = Y_data_max.drop(Y_data_max.columns[-18:], axis=1)\n",
    "# Y_data_max = df_new.drop(df_new.columns[17], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60853177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = Y_data_extra.drop(Y_data_extra.columns[-18:], axis=1)\n",
    "# Y_data_extra = df_new.drop(df_new.columns[17], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RealCase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a334a6d8",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad7c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"RealCase_Y_train.csv\", Y_data_train, delimiter=\",\", fmt=\"%.8f\")\n",
    "np.savetxt(\"RealCase_Y_test.csv\", Y_data_test, delimiter=\",\", fmt=\"%.8f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38e4e8",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "((RealCase.abs().values >= Y_data_max.abs().values)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927c0486",
   "metadata": {},
   "source": [
    "# Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb6d8240",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "Y_data_train_standardized = pd.DataFrame(scaler.fit_transform(Y_data_train), columns=Y_data_train.columns).values\n",
    "\n",
    "Y_data_test_standardized = pd.DataFrame(scaler.transform(Y_data_test), columns=Y_data_test.columns).values\n",
    "\n",
    "RealCase_standardized = pd.DataFrame(scaler.transform(RealCase), columns=RealCase.columns).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b26e1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"RealCase_Y_train_std.csv\", Y_data_train_standardized, delimiter=\",\", fmt=\"%.8f\")\n",
    "np.savetxt(\"RealCase_Y_test_std.csv\", Y_data_test_standardized, delimiter=\",\", fmt=\"%.8f\")\n",
    "np.savetxt(\"RealCase_Y_std.csv\", RealCase_standardized, delimiter=\",\", fmt=\"%.8f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7efe3",
   "metadata": {},
   "source": [
    "# Mapping back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de634685",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../Data/Data20260110/X_train_Final.csv', header=None, delimiter=',').values\n",
    "\n",
    "\n",
    "m = X_train.shape[0]\n",
    "\n",
    "mapping = {\n",
    "    0: [4, 16],\n",
    "    1: [5, 17],\n",
    "    2: [10, 22],\n",
    "    3: [11, 23],\n",
    "    4: [8, 6, 20, 18],\n",
    "    5: [9, 7, 21, 19],\n",
    "    6: [2, 0, 14, 12],\n",
    "    7: [3, 1, 15, 13],\n",
    "    8: [24, 26, 28, 30, 32],\n",
    "    9: [25, 27, 29, 31, 33]\n",
    "}\n",
    "\n",
    "X_all_recovered = np.zeros((m, len(mapping)))\n",
    "\n",
    "\n",
    "for orig_col, new_cols in mapping.items():\n",
    "\n",
    "    X_all_recovered[:, orig_col] = X_train[:, new_cols].mean(axis=1)\n",
    "\n",
    "X_train = np.around(X_all_recovered, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('../Data/X_test.txt', header=None, delimiter=' ').values\n",
    "\n",
    "\n",
    "m = X_test.shape[0]\n",
    "\n",
    "mapping = {\n",
    "    0: [4, 16],\n",
    "    1: [5, 17],\n",
    "    2: [10, 22],\n",
    "    3: [11, 23],\n",
    "    4: [8, 6, 20, 18],\n",
    "    5: [9, 7, 21, 19],\n",
    "    6: [2, 0, 14, 12],\n",
    "    7: [3, 1, 15, 13],\n",
    "    8: [24, 26, 28, 30, 32],\n",
    "    9: [25, 27, 29, 31, 33]\n",
    "}\n",
    "\n",
    "X_all_recovered = np.zeros((m, len(mapping)))\n",
    "\n",
    "\n",
    "for orig_col, new_cols in mapping.items():\n",
    "\n",
    "    X_all_recovered[:, orig_col] = X_test[:, new_cols].mean(axis=1)\n",
    "\n",
    "X_test = np.around(X_all_recovered, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2bce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"RealCase_X_train.csv\", X_train, delimiter=\",\", fmt=\"%.4f\")\n",
    "np.savetxt(\"RealCase_X_test.csv\", X_test, delimiter=\",\", fmt=\"%.4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9891d",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b0eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sign_flip_scores(components, scores):\n",
    "\n",
    "    comps = components.copy()\n",
    "    Z = scores.copy()\n",
    "    for i in range(comps.shape[0]):\n",
    "        j = np.argmax(np.abs(comps[i]))  # 该成分绝对值最大的载荷索引\n",
    "        if comps[i, j] < 0:              # 若为负则整体翻转\n",
    "            comps[i] *= -1\n",
    "            Z[:, i] *= -1\n",
    "    return comps, Z\n",
    "\n",
    "def split_and_apply_pca(train_data, test_data, variance_threshold=0.999,\n",
    "                        svd_solver='full', random_state=0):\n",
    "\n",
    "    # 1) 拆分第一列\n",
    "    train_first_col = train_data[:, 0].reshape(-1, 1)\n",
    "    test_first_col  = test_data[:, 0].reshape(-1, 1)\n",
    "\n",
    "    train_remaining = train_data[:, 1:]\n",
    "    test_remaining  = test_data[:, 1:]\n",
    "\n",
    "    # 2) 先用完整 PCA 拟合以取累计方差（确定性求解器 & 固定 random_state）\n",
    "    pca_full = PCA(svd_solver=svd_solver, random_state=random_state)\n",
    "    pca_full.fit(train_remaining)\n",
    "    cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "    # 3) 选取主成分个数\n",
    "    n_components = int(np.argmax(cumulative_variance >= variance_threshold) + 1)\n",
    "\n",
    "    # 4) 用 n_components 重新拟合（同样保证确定性）\n",
    "    pca = PCA(n_components=n_components, svd_solver=svd_solver, random_state=random_state)\n",
    "    train_scores = pca.fit_transform(train_remaining)  # Z_train\n",
    "    test_scores  = pca.transform(test_remaining)       # Z_test\n",
    "\n",
    "    # 5) 进行“符号固定”，消除 ± 号的随机性\n",
    "    comps_fixed, train_scores_fixed = _sign_flip_scores(pca.components_, train_scores)\n",
    "    _,           test_scores_fixed  = _sign_flip_scores(pca.components_, test_scores)\n",
    "\n",
    "    # （可选）若你希望把固定后的 components 回写给 pca 对象，可取消下面两行注释：\n",
    "    # pca.components_ = comps_fixed\n",
    "    # 注意：scikit-learn 并不依赖 components_ 的符号唯一性，回写仅用于记录\n",
    "\n",
    "    # 6) 合并回第一列\n",
    "    train_final = np.hstack((train_first_col, train_scores_fixed))\n",
    "    test_final  = np.hstack((test_first_col,  test_scores_fixed))\n",
    "\n",
    "    return train_final, test_final, n_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1882d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_final, Y_test_final, n_components = split_and_apply_pca(\n",
    "    Y_data_train_standardized,\n",
    "    RealCase_standardized,\n",
    "    variance_threshold=0.999\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a28f6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217fdbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_min = np.nanmin(Y_train_final, axis=0)\n",
    "col_max = np.nanmax(Y_train_final, axis=0)\n",
    "\n",
    "\n",
    "((Y_test_final <= col_min) & (Y_test_final >= col_max)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb9ce0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"RealCase_Y_pca.csv\", Y_test_final, delimiter=\",\", fmt=\"%.8f\")\n",
    "np.savetxt(\"RealCase_Y_train_pca.csv\", Y_train_final, delimiter=\",\", fmt=\"%.8f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"RealCase_Y_test_pca.csv\", Y_test_final, delimiter=\",\", fmt=\"%.8f\")\n",
    "np.savetxt(\"RealCase_Y_train_pca.csv\", Y_train_final, delimiter=\",\", fmt=\"%.8f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e987fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a7bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75125323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_grid(Y, real, start=0, num=24, ncols=6, bins=30):\n",
    "    real = np.asarray(real).squeeze()\n",
    "    if real.ndim == 0:\n",
    "        real = real[None]\n",
    "\n",
    "    p = Y.shape[1]\n",
    "    end = min(p, start + num)\n",
    "    idxs = list(range(start, end))\n",
    "\n",
    "    nrows = math.ceil(len(idxs) / ncols)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(3.2*ncols, 2.4*nrows), squeeze=False)\n",
    "\n",
    "    for k, j in enumerate(idxs):\n",
    "        r, c = divmod(k, ncols)\n",
    "        ax = axes[r][c]\n",
    "        ax.hist(Y[:, j], bins=bins)\n",
    "        ax.axvline(real[j])  # RealCase 的位置\n",
    "        ax.set_title(f\"col {j}\")\n",
    "        ax.grid(alpha=0.2)\n",
    "\n",
    "    # 多余子图关掉\n",
    "    for k in range(len(idxs), nrows*ncols):\n",
    "        r, c = divmod(k, ncols)\n",
    "        axes[r][c].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# real = Y_test_final.squeeze()\n",
    "plot_hist_grid(Y_train_final, Y_test_final, start=0,  num=33)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aafa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def _sign_flip_components_and_scores(components, scores):\n",
    "    \"\"\"\n",
    "    固定PCA符号：对每个PC，找到绝对值最大的载荷所在特征维 j；\n",
    "    若该载荷为负，则整体翻转该PC的components与scores。\n",
    "    返回：comps_fixed, scores_fixed, signs(长度=K, 元素为±1)\n",
    "    \"\"\"\n",
    "    comps = components.copy()              # (K, D)\n",
    "    K = comps.shape[0]\n",
    "    signs = np.ones(K, dtype=float)\n",
    "\n",
    "    for i in range(K):\n",
    "        j = np.argmax(np.abs(comps[i]))\n",
    "        if comps[i, j] < 0:\n",
    "            signs[i] = -1.0\n",
    "            comps[i] *= -1.0\n",
    "\n",
    "    Z_fixed = scores * signs[None, :]      # (N, K)\n",
    "    return comps, Z_fixed, signs\n",
    "\n",
    "\n",
    "def _pca_coverage_detection(\n",
    "    train_scores_fixed,\n",
    "    test_scores_fixed,\n",
    "    comps_fixed,\n",
    "    pca_obj,\n",
    "    train_remaining,\n",
    "    test_remaining,\n",
    "    q_low=0.005,\n",
    "    q_high=0.995,\n",
    "    topk_features=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    基于训练集PC得分分位数边界检测测试集超界，并将超界部分反投影回原始特征维。\n",
    "\n",
    "    返回 report 字典，包含：\n",
    "      - bounds_low/high: (K,)\n",
    "      - pc_out_mask: (n_test, K) bool\n",
    "      - delta_z: (n_test, K)\n",
    "      - delta_x_centered: (n_test, D_remaining)  # 原始特征空间(中心化后)的“驱动偏离量”\n",
    "      - per_sample_top_features: list[dict]      # 每个测试样本的topk原始维贡献\n",
    "      - global_top_features: list[tuple]         # 全局(在超界样本上)平均贡献排序\n",
    "      - (optional) T2/Q 及其特征贡献\n",
    "    \"\"\"\n",
    "    Ztr = train_scores_fixed\n",
    "    Zte = test_scores_fixed\n",
    "    K = Ztr.shape[1]\n",
    "    Drem = train_remaining.shape[1]\n",
    "\n",
    "    # 1) 训练覆盖边界（按PC维）\n",
    "    bounds_low = np.quantile(Ztr, q_low, axis=0)   # (K,)\n",
    "    bounds_high = np.quantile(Ztr, q_high, axis=0) # (K,)\n",
    "\n",
    "    # 2) 哪些测试样本在哪些PC维超界\n",
    "    pc_out_mask = (Zte < bounds_low[None, :]) | (Zte > bounds_high[None, :])  # (n_test, K)\n",
    "    any_out = pc_out_mask.any(axis=1)                                        # (n_test,)\n",
    "\n",
    "    # 3) 计算超界部分 delta_z（只保留超界“超出去”的那一截）\n",
    "    Zte_clipped = np.clip(Zte, bounds_low[None, :], bounds_high[None, :])\n",
    "    delta_z = Zte - Zte_clipped  # (n_test, K) 超界维非零\n",
    "\n",
    "    # 4) 将 delta_z 反投影到原始特征空间（中心化空间）\n",
    "    # components 形状 (K, Drem)，故 delta_x = delta_z @ components -> (n_test, Drem)\n",
    "    delta_x_centered = delta_z @ comps_fixed\n",
    "\n",
    "    # 5) 每个测试样本：原始维贡献TopK（按 |delta_x| 排序）\n",
    "    per_sample_top = []\n",
    "    for i in range(Zte.shape[0]):\n",
    "        if not any_out[i]:\n",
    "            per_sample_top.append({\n",
    "                \"sample_idx\": i,\n",
    "                \"out_pcs\": [],\n",
    "                \"top_features\": []\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        out_pcs = np.where(pc_out_mask[i])[0].tolist()\n",
    "        contrib = delta_x_centered[i]  # (Drem,)\n",
    "        order = np.argsort(-np.abs(contrib))[:topk_features]\n",
    "\n",
    "        # 注意：这里的 feature_idx 是 “remaining” 内部索引 0..Drem-1\n",
    "        # 如果你要映射回原 train_data 的原始列：原始列号 = feature_idx + 1（因为第0列拆出）\n",
    "        top_feats = [{\n",
    "            \"feature_idx_remaining\": int(j),\n",
    "            \"feature_idx_original\": int(j + 1),\n",
    "            \"contrib\": float(contrib[j]),\n",
    "            \"abs_contrib\": float(abs(contrib[j]))\n",
    "        } for j in order]\n",
    "\n",
    "        per_sample_top.append({\n",
    "            \"sample_idx\": i,\n",
    "            \"out_pcs\": out_pcs,\n",
    "            \"top_features\": top_feats\n",
    "        })\n",
    "\n",
    "    # 6) 全局：在“超界样本”上按平均 |delta_x| 排序\n",
    "    if np.any(any_out):\n",
    "        mean_abs = np.mean(np.abs(delta_x_centered[any_out]), axis=0)  # (Drem,)\n",
    "        order_g = np.argsort(-mean_abs)[:topk_features]\n",
    "        global_top = [{\n",
    "            \"feature_idx_remaining\": int(j),\n",
    "            \"feature_idx_original\": int(j + 1),\n",
    "            \"mean_abs_contrib\": float(mean_abs[j])\n",
    "        } for j in order_g]\n",
    "    else:\n",
    "        global_top = []\n",
    "\n",
    "    # 7) （可选）T^2 与 Q(SPE) 及其特征贡献：帮助区分“子空间内远离”vs“子空间外新结构”\n",
    "    # PCA 的 transform: Z = (X - mean) @ components.T\n",
    "    # 在中心化空间：Xc = X - mean，Xhat_c = Z @ components\n",
    "    Xc_te = test_remaining - pca_obj.mean_[None, :]        # (n_test, Drem)\n",
    "    Xhat_c = Zte @ comps_fixed                              # (n_test, Drem)\n",
    "    resid = Xc_te - Xhat_c\n",
    "    Q = np.sum(resid**2, axis=1)                            # (n_test,)\n",
    "    c_Q = resid**2                                          # (n_test, Drem) 逐特征贡献\n",
    "\n",
    "    lam = np.maximum(pca_obj.explained_variance_, 1e-12)    # (K,)\n",
    "    T2 = np.sum((Zte**2) / lam[None, :], axis=1)            # (n_test,)\n",
    "\n",
    "    # T^2 二次型分解：T2 = x^T M x, 贡献 c_T2_i = x_i (Mx)_i\n",
    "    P = comps_fixed.T                                       # (Drem, K)\n",
    "    M = P @ np.diag(1.0 / lam) @ P.T                        # (Drem, Drem)\n",
    "    Mx = Xc_te @ M.T                                        # (n_test, Drem)\n",
    "    c_T2 = Xc_te * Mx                                       # (n_test, Drem)\n",
    "\n",
    "    report = {\n",
    "        \"q_low\": q_low,\n",
    "        \"q_high\": q_high,\n",
    "        \"bounds_low\": bounds_low,\n",
    "        \"bounds_high\": bounds_high,\n",
    "        \"pc_out_mask\": pc_out_mask,\n",
    "        \"any_out\": any_out,\n",
    "        \"delta_z\": delta_z,\n",
    "        \"delta_x_centered\": delta_x_centered,\n",
    "        \"per_sample_top_features\": per_sample_top,\n",
    "        \"global_top_features\": global_top,\n",
    "        \"T2\": T2,\n",
    "        \"c_T2\": c_T2,\n",
    "        \"Q\": Q,\n",
    "        \"c_Q\": c_Q,\n",
    "    }\n",
    "    return report\n",
    "\n",
    "\n",
    "def split_and_apply_pca(\n",
    "    train_data,\n",
    "    test_data,\n",
    "    variance_threshold=0.999,\n",
    "    svd_solver='full',\n",
    "    random_state=0,\n",
    "    # --- detection config ---\n",
    "    return_report=False,\n",
    "    q_low=0.005,\n",
    "    q_high=0.995,\n",
    "    topk_features=10\n",
    "):\n",
    "    # 1) 拆分第一列\n",
    "    train_first_col = train_data[:, 0].reshape(-1, 1)\n",
    "    test_first_col  = test_data[:, 0].reshape(-1, 1)\n",
    "\n",
    "    train_remaining = train_data[:, 1:]\n",
    "    test_remaining  = test_data[:, 1:]\n",
    "\n",
    "    # 2) 先用完整 PCA 拟合以取累计方差（确定性求解器 & 固定 random_state）\n",
    "    pca_full = PCA(svd_solver=svd_solver, random_state=random_state)\n",
    "    pca_full.fit(train_remaining)\n",
    "    cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "    # 3) 选取主成分个数\n",
    "    n_components = int(np.argmax(cumulative_variance >= variance_threshold) + 1)\n",
    "\n",
    "    # 4) 用 n_components 重新拟合（同样保证确定性）\n",
    "    pca = PCA(n_components=n_components, svd_solver=svd_solver, random_state=random_state)\n",
    "    train_scores = pca.fit_transform(train_remaining)  # Z_train\n",
    "    test_scores  = pca.transform(test_remaining)       # Z_test\n",
    "\n",
    "    # 5) 进行“符号固定”，消除 ± 号的随机性（关键：test 用同一 signs）\n",
    "    comps_fixed, train_scores_fixed, signs = _sign_flip_components_and_scores(pca.components_, train_scores)\n",
    "    test_scores_fixed = test_scores * signs[None, :]\n",
    "\n",
    "    # 6) 合并回第一列\n",
    "    train_final = np.hstack((train_first_col, train_scores_fixed))\n",
    "    test_final  = np.hstack((test_first_col,  test_scores_fixed))\n",
    "\n",
    "    if not return_report:\n",
    "        return train_final, test_final, n_components\n",
    "\n",
    "    # 7) 检测：测试集哪些PC超出训练覆盖，并归因到原始维\n",
    "    report = _pca_coverage_detection(\n",
    "        train_scores_fixed=train_scores_fixed,\n",
    "        test_scores_fixed=test_scores_fixed,\n",
    "        comps_fixed=comps_fixed,\n",
    "        pca_obj=pca,\n",
    "        train_remaining=train_remaining,\n",
    "        test_remaining=test_remaining,\n",
    "        q_low=q_low,\n",
    "        q_high=q_high,\n",
    "        topk_features=topk_features,\n",
    "    )\n",
    "\n",
    "    return train_final, test_final, n_components, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f3050",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca, test_pca, k, report = split_and_apply_pca(\n",
    "    Y_data_train_standardized, RealCase_standardized,\n",
    "    variance_threshold=0.999,\n",
    "    return_report=True,\n",
    "    q_low=0.005, q_high=0.995,\n",
    "    topk_features=8\n",
    ")\n",
    "\n",
    "# 1) 哪些测试样本在PC上超界\n",
    "out_samples = np.where(report[\"any_out\"])[0]          # 测试集中“有任一PC超界”的样本索引\n",
    "pc_out_mask = report[\"pc_out_mask\"]                   # (n_test, K) bool\n",
    "\n",
    "# 2) 全局：最驱动偏离的原始特征（按 mean |delta_x|）\n",
    "report[\"global_top_features\"]\n",
    "# 每个元素包含 feature_idx_original（对应 train_data 的列号）和 mean_abs_contrib\n",
    "\n",
    "# 3) 单样本：i 号测试样本具体是哪些原始维导致偏离\n",
    "i = int(out_samples[0])\n",
    "report[\"per_sample_top_features\"][i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3fb012",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPTG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
