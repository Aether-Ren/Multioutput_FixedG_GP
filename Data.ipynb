{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b2e7353",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced48ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import GP_functions.Tools as Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa60f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans2\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import qmc, multivariate_normal\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9c04a",
   "metadata": {},
   "source": [
    "# Create para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_10d_grouped_sobol_scipy(n_per_group=200, low=0.1, high=5.0, base_seed=1017):\n",
    "\n",
    "    assert n_per_group > 0 and low < high\n",
    "    groups = [(0,1), (2,3), (4,5), (6,7), (8,9)]\n",
    "    batches = []\n",
    "\n",
    "    for g_idx, (c1, c2) in enumerate(groups):\n",
    "\n",
    "        Xg = np.ones((n_per_group, 10), dtype=float)\n",
    "\n",
    "  \n",
    "        eng = qmc.Sobol(d=2, scramble=True, seed=base_seed + g_idx)\n",
    "        U = eng.random(n_per_group)\n",
    "\n",
    "\n",
    "        S = qmc.scale(U, l_bounds=[low, low], u_bounds=[high, high])\n",
    "\n",
    "\n",
    "        Xg[:, c1:c2+1] = S\n",
    "        batches.append(Xg)\n",
    "\n",
    "    X = np.vstack(batches)  # (5*n_per_group, 10) = (1000, 10)\n",
    "    return X\n",
    "\n",
    "# 生成数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = generate_10d_grouped_sobol_scipy(n_per_group=128, low=0.1, high=5.0, base_seed=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X_all.shape\n",
    "\n",
    "\n",
    "new_data = np.zeros((m, 34))\n",
    "\n",
    "\n",
    "mapping = {\n",
    "    0: [4, 16],\n",
    "    1: [5, 17],\n",
    "    2: [10, 22],\n",
    "    3: [11, 23],\n",
    "    4: [8, 6, 20, 18],\n",
    "    5: [9, 7, 21, 19],\n",
    "    6: [2, 0, 14, 12],\n",
    "    7: [3, 1, 15, 13],\n",
    "    8: [24, 26, 28, 30, 32],\n",
    "    9: [25, 27, 29, 31, 33]\n",
    "}\n",
    "\n",
    "for orig_col, new_cols in mapping.items():\n",
    "    for new_col in new_cols:\n",
    "        new_data[:, new_col] = X_all[:, orig_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0743d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"X_edge.txt\", new_data, fmt='%0.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypercube_vertices_10d_product(low=0.1, high=5.0, d=10):\n",
    "    return np.array(list(product([low, high], repeat=d)), dtype=float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corner = hypercube_vertices_10d_product()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b27bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = X_corner\n",
    "\n",
    "m, n = X_all.shape\n",
    "\n",
    "\n",
    "new_data = np.zeros((m, 34))\n",
    "\n",
    "\n",
    "mapping = {\n",
    "    0: [4, 16],\n",
    "    1: [5, 17],\n",
    "    2: [10, 22],\n",
    "    3: [11, 23],\n",
    "    4: [8, 6, 20, 18],\n",
    "    5: [9, 7, 21, 19],\n",
    "    6: [2, 0, 14, 12],\n",
    "    7: [3, 1, 15, 13],\n",
    "    8: [24, 26, 28, 30, 32],\n",
    "    9: [25, 27, 29, 31, 33]\n",
    "}\n",
    "\n",
    "for orig_col, new_cols in mapping.items():\n",
    "    for new_col in new_cols:\n",
    "        new_data[:, new_col] = X_all[:, orig_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4721be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"X_corner.txt\", new_data, fmt='%0.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d616a",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a28d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data_train = pd.read_csv('Data/simulation_results_train.txt', header=None, delimiter=' ')\n",
    "Y_data_train = Y_data_train.drop(columns=[Y_data_train.columns[-1]])\n",
    "\n",
    "\n",
    "Y_data_test = pd.read_csv('Data/simulation_results_test.txt', header=None, delimiter=' ')\n",
    "Y_data_test = Y_data_test.drop(columns=[Y_data_test.columns[-1]])\n",
    "\n",
    "Y_data_1 = pd.read_csv('LocalDisease/simulation_results_X_1.txt', header=None, delimiter=' ')\n",
    "Y_data_1 = Y_data_1.drop(columns=[Y_data_1.columns[-1]])\n",
    "\n",
    "Y_data_1_1 = pd.read_csv('LocalDisease/simulation_results_X_1_1.txt', header=None, delimiter=' ')\n",
    "Y_data_1_1 = Y_data_1_1.drop(columns=[Y_data_1_1.columns[-1]])\n",
    "\n",
    "Y_data_1_5 = pd.read_csv('LocalDisease/simulation_results_X_1_5.txt', header=None, delimiter=' ')\n",
    "Y_data_1_5 = Y_data_1_5.drop(columns=[Y_data_1_5.columns[-1]])\n",
    "\n",
    "Y_data_2 = pd.read_csv('LocalDisease/simulation_results_X_2.txt', header=None, delimiter=' ')\n",
    "Y_data_2 = Y_data_2.drop(columns=[Y_data_2.columns[-1]])\n",
    "\n",
    "Y_data_edge = pd.read_csv('Data/simulation_results_X_edge.txt', header=None, delimiter=' ')\n",
    "Y_data_edge = Y_data_edge.drop(columns=[Y_data_edge.columns[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723a09df",
   "metadata": {},
   "source": [
    "# Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe53e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "Y_data_train_standardized = pd.DataFrame(scaler.fit_transform(Y_data_train), columns=Y_data_train.columns).values\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "Y_data_test_standardized = pd.DataFrame(scaler.transform(Y_data_test), columns=Y_data_test.columns).values\n",
    "\n",
    "Y_data_1_standardized = pd.DataFrame(scaler.transform(Y_data_1), columns=Y_data_1.columns).values\n",
    "\n",
    "Y_data_1_1_standardized = pd.DataFrame(scaler.transform(Y_data_1_1), columns=Y_data_1_1.columns).values\n",
    "\n",
    "Y_data_1_5_standardized = pd.DataFrame(scaler.transform(Y_data_1_5), columns=Y_data_1_5.columns).values\n",
    "\n",
    "Y_data_2_standardized = pd.DataFrame(scaler.transform(Y_data_2), columns=Y_data_2.columns).values\n",
    "\n",
    "Y_data_edge_standardized = pd.DataFrame(scaler.transform(Y_data_edge), columns=Y_data_edge.columns).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cfc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_min = np.nanmin(Y_data_edge_standardized, axis=0)\n",
    "col_max = np.nanmax(Y_data_edge_standardized, axis=0)\n",
    "\n",
    "\n",
    "((Y_data_1_1_standardized >= col_min) & (Y_data_1_1_standardized <= col_max)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c492d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = (Y_data_1_5_standardized >= col_min) & (Y_data_1_5_standardized <= col_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67573ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff67f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"LocalDisease/Y_data_1_5_std.csv\", Y_data_1_5_standardized, delimiter=\",\", fmt=\"%.8f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996bf7e",
   "metadata": {},
   "source": [
    "# Mapping back X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dee7dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('Data/X_test.txt', header=None, delimiter=' ').values\n",
    "\n",
    "\n",
    "m = X_test.shape[0]\n",
    "\n",
    "mapping = {\n",
    "    0: [4, 16],\n",
    "    1: [5, 17],\n",
    "    2: [10, 22],\n",
    "    3: [11, 23],\n",
    "    4: [8, 6, 20, 18],\n",
    "    5: [9, 7, 21, 19],\n",
    "    6: [2, 0, 14, 12],\n",
    "    7: [3, 1, 15, 13],\n",
    "    8: [24, 26, 28, 30, 32],\n",
    "    9: [25, 27, 29, 31, 33]\n",
    "}\n",
    "\n",
    "X_all_recovered = np.zeros((m, len(mapping)))\n",
    "\n",
    "\n",
    "for orig_col, new_cols in mapping.items():\n",
    "\n",
    "    X_all_recovered[:, orig_col] = X_test[:, new_cols].mean(axis=1)\n",
    "\n",
    "X_test = np.around(X_all_recovered, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5c36e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Data/X_train.txt', header=None, delimiter=' ').head(Y_data_train_standardized.shape[0]).values\n",
    "\n",
    "\n",
    "m = X_train.shape[0]\n",
    "\n",
    "mapping = {\n",
    "    0: [4, 16],\n",
    "    1: [5, 17],\n",
    "    2: [10, 22],\n",
    "    3: [11, 23],\n",
    "    4: [8, 6, 20, 18],\n",
    "    5: [9, 7, 21, 19],\n",
    "    6: [2, 0, 14, 12],\n",
    "    7: [3, 1, 15, 13],\n",
    "    8: [24, 26, 28, 30, 32],\n",
    "    9: [25, 27, 29, 31, 33]\n",
    "}\n",
    "\n",
    "X_all_recovered = np.zeros((m, len(mapping)))\n",
    "\n",
    "\n",
    "for orig_col, new_cols in mapping.items():\n",
    "\n",
    "    X_all_recovered[:, orig_col] = X_train[:, new_cols].mean(axis=1)\n",
    "\n",
    "X_train = np.around(X_all_recovered, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b765833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1_5 = pd.read_csv('LocalDisease/X_1_5.txt', header=None, delimiter=' ').values\n",
    "\n",
    "\n",
    "m = X_1_5.shape[0]\n",
    "\n",
    "mapping = {\n",
    "    0: [4, 16],\n",
    "    1: [5, 17],\n",
    "    2: [10, 22],\n",
    "    3: [11, 23],\n",
    "    4: [8, 6, 20, 18],\n",
    "    5: [9, 7, 21, 19],\n",
    "    6: [2, 0, 14, 12],\n",
    "    7: [3, 1, 15, 13],\n",
    "    8: [24, 26, 28, 30, 32],\n",
    "    9: [25, 27, 29, 31, 33]\n",
    "}\n",
    "\n",
    "X_all_recovered = np.zeros((m, len(mapping)))\n",
    "\n",
    "\n",
    "for orig_col, new_cols in mapping.items():\n",
    "\n",
    "    X_all_recovered[:, orig_col] = X_1_5[:, new_cols].mean(axis=1)\n",
    "\n",
    "X_1_5 = np.around(X_all_recovered, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36fe6398",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"LocalDisease/X_1_5.csv\", X_1_5, delimiter=\",\", fmt=\"%.4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648415c",
   "metadata": {},
   "source": [
    "# Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29ec95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices_train = Tools.get_outlier_indices_iqr(Y_data_train_standardized, outbound = 6.5)\n",
    "outlier_indices_test = Tools.get_outlier_indices_iqr(Y_data_test_standardized, outbound = 6.5)\n",
    "\n",
    "Y_data_train_standardized = np.delete(Y_data_train_standardized, outlier_indices_train, axis=0)\n",
    "X_train = np.delete(X_train, outlier_indices_train, axis=0)\n",
    "\n",
    "Y_data_test_standardized = np.delete(Y_data_test_standardized, outlier_indices_test, axis=0)\n",
    "X_test = np.delete(X_test, outlier_indices_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fff7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "front_indices = np.arange(6)\n",
    "back_indices = np.arange(-380, 0)\n",
    "\n",
    "X_test_front = X_test[front_indices]\n",
    "Y_test_front = Y_data_test_standardized[front_indices]\n",
    "X_test_back = X_test[back_indices]\n",
    "Y_test_back = Y_data_test_standardized[back_indices]\n",
    "\n",
    "X_train = np.concatenate((X_train, X_test_front, X_test_back), axis=0)\n",
    "Y_data_train_standardized = np.concatenate((Y_data_train_standardized, Y_test_front, Y_test_back), axis=0)\n",
    "\n",
    "X_test = np.delete(X_test, np.concatenate((front_indices, back_indices)), axis=0)\n",
    "Y_data_test_standardized = np.delete(Y_data_test_standardized, np.concatenate((front_indices, back_indices)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = Y_data_test_standardized[:, 1:]  # 去掉第一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb7d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.isin(np.arange(B.shape[1]) % 17, [2, 8])  # 每17列一个周期，取周期内第3、9列(0-based: 2,8)\n",
    "out = B[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a205ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"X_train_anterior.csv\", X_train[:,0:2], delimiter=\",\", fmt=\"%.8f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00378973",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ddf701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sign_flip_scores(components, scores):\n",
    "\n",
    "    comps = components.copy()\n",
    "    Z = scores.copy()\n",
    "    for i in range(comps.shape[0]):\n",
    "        j = np.argmax(np.abs(comps[i]))  # 该成分绝对值最大的载荷索引\n",
    "        if comps[i, j] < 0:              # 若为负则整体翻转\n",
    "            comps[i] *= -1\n",
    "            Z[:, i] *= -1\n",
    "    return comps, Z\n",
    "\n",
    "def split_and_apply_pca(train_data, test_data, variance_threshold=0.999,\n",
    "                        svd_solver='full', random_state=0):\n",
    "\n",
    "    # 1) 拆分第一列\n",
    "    train_first_col = train_data[:, 0].reshape(-1, 1)\n",
    "    test_first_col  = test_data[:, 0].reshape(-1, 1)\n",
    "\n",
    "    train_remaining = train_data[:, 1:]\n",
    "    test_remaining  = test_data[:, 1:]\n",
    "\n",
    "    # 2) 先用完整 PCA 拟合以取累计方差（确定性求解器 & 固定 random_state）\n",
    "    pca_full = PCA(svd_solver=svd_solver, random_state=random_state)\n",
    "    pca_full.fit(train_remaining)\n",
    "    cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "    # 3) 选取主成分个数\n",
    "    n_components = int(np.argmax(cumulative_variance >= variance_threshold) + 1)\n",
    "\n",
    "    # 4) 用 n_components 重新拟合（同样保证确定性）\n",
    "    pca = PCA(n_components=n_components, svd_solver=svd_solver, random_state=random_state)\n",
    "    train_scores = pca.fit_transform(train_remaining)  # Z_train\n",
    "    test_scores  = pca.transform(test_remaining)       # Z_test\n",
    "\n",
    "    # 5) 进行“符号固定”，消除 ± 号的随机性\n",
    "    comps_fixed, train_scores_fixed = _sign_flip_scores(pca.components_, train_scores)\n",
    "    _,           test_scores_fixed  = _sign_flip_scores(pca.components_, test_scores)\n",
    "\n",
    "    # （可选）若你希望把固定后的 components 回写给 pca 对象，可取消下面两行注释：\n",
    "    # pca.components_ = comps_fixed\n",
    "    # 注意：scikit-learn 并不依赖 components_ 的符号唯一性，回写仅用于记录\n",
    "\n",
    "    # 6) 合并回第一列\n",
    "    train_final = np.hstack((train_first_col, train_scores_fixed))\n",
    "    test_final  = np.hstack((test_first_col,  test_scores_fixed))\n",
    "\n",
    "    return train_final, test_final, n_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6fbb68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_final, Y_test_final, n_components = split_and_apply_pca(\n",
    "    Y_data_train_standardized,\n",
    "    Y_data_1_5_standardized,\n",
    "    variance_threshold=0.999\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f23b83b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0842285",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b54a1c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"X_train.csv\", X_train, delimiter=\",\", fmt=\"%.4f\")\n",
    "# np.savetxt(\"X_test.csv\", X_test, delimiter=\",\", fmt=\"%.4f\")\n",
    "\n",
    "# np.savetxt(\"LocalDisease/Y_train_std_pca.csv\", Y_train_final, delimiter=\",\", fmt=\"%.8f\")\n",
    "np.savetxt(\"LocalDisease/Y_data_1_5_pca.csv\", Y_test_final, delimiter=\",\", fmt=\"%.8f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4b8e15",
   "metadata": {},
   "source": [
    "LocalDisease/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5eba6c",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPTG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
