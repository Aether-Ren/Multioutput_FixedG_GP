{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265579b4",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f23c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "from linear_operator import settings\n",
    "\n",
    "import pyro\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68cd65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GP_functions.Loss_function as Loss_function\n",
    "import GP_functions.bound as bound\n",
    "import GP_functions.Estimation as Estimation\n",
    "import GP_functions.Training as Training\n",
    "import GP_functions.Prediction as Prediction\n",
    "import GP_functions.GP_models as GP_models\n",
    "import GP_functions.Tools as Tools\n",
    "import GP_functions.FeatureE as FeatureE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdfbef9",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2741d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Data/X_train.csv', header=None, delimiter=',').values\n",
    "X_test = pd.read_csv('Data/X_test.csv', header=None, delimiter=',').values\n",
    "\n",
    "Y_train_21 = pd.read_csv('Data/Y_train_std_21.csv', header=None, delimiter=',').values\n",
    "Y_test_21 = pd.read_csv('Data/Y_test_std_21.csv', header=None, delimiter=',').values\n",
    "\n",
    "Y_train_std = pd.read_csv('Data/Y_train_std.csv', header=None, delimiter=',').values\n",
    "Y_test_std = pd.read_csv('Data/Y_test_std.csv', header=None, delimiter=',').values\n",
    "\n",
    "\n",
    "train_x = torch.tensor(X_train, dtype=torch.float32)\n",
    "test_x = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "train_y_21 = torch.tensor(Y_train_21, dtype=torch.float32)\n",
    "test_y_21 = torch.tensor(Y_test_21, dtype=torch.float32)\n",
    "\n",
    "# train_y = torch.tensor(Y_train_std, dtype=torch.float32)\n",
    "# test_y = torch.tensor(Y_test_std, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe41da",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGPHiddenLayer(gpytorch.models.deep_gps.DeepGPLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims,\n",
    "        output_dims,\n",
    "        num_inducing = 512,\n",
    "        covar_type = \"RBF\",\n",
    "        linear_mean = False,\n",
    "        train_x_for_init = None\n",
    "    ):\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        batch_shape = torch.Size([output_dims])\n",
    "\n",
    "        if train_x_for_init is not None:\n",
    "            idx = torch.randperm(train_x_for_init.size(0))[:num_inducing]\n",
    "            inducing_points = train_x_for_init[idx].clone()\n",
    "            inducing_points = inducing_points.unsqueeze(0).expand(\n",
    "                output_dims, -1, -1\n",
    "            )  # B x M x D\n",
    "        else:\n",
    "            inducing_points = (\n",
    "                torch.rand(output_dims, num_inducing, input_dims) * 4.9 + 0.1\n",
    "            )\n",
    "\n",
    "        variational_dist = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_dist,\n",
    "            learn_inducing_locations=True,\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy, input_dims, output_dims)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.ZeroMean() if linear_mean else gpytorch.means.LinearMean(input_dims)\n",
    "        \n",
    "        if covar_type == 'Matern5/2':\n",
    "            base_kernel = gpytorch.kernels.MaternKernel(nu=2.5,\n",
    "                                                        batch_shape=batch_shape,\n",
    "                                                        ard_num_dims=input_dims)\n",
    "        elif covar_type == 'RBF':\n",
    "            base_kernel = gpytorch.kernels.RBFKernel(batch_shape=batch_shape,\n",
    "                                                     ard_num_dims=input_dims)\n",
    "        elif covar_type == 'Matern3/2':\n",
    "            base_kernel = gpytorch.kernels.MaternKernel(nu=1.5,\n",
    "                                                        batch_shape=batch_shape,\n",
    "                                                        ard_num_dims=input_dims)\n",
    "        elif covar_type == 'RQ':\n",
    "            base_kernel = gpytorch.kernels.RQKernel(batch_shape=batch_shape,\n",
    "                                                    ard_num_dims=input_dims)\n",
    "        elif covar_type == 'PiecewisePolynomial':\n",
    "            base_kernel = gpytorch.kernels.PiecewisePolynomialKernel(q=2,\n",
    "                                                                     batch_shape=batch_shape,\n",
    "                                                                     ard_num_dims=input_dims)\n",
    "        else:\n",
    "            raise ValueError(\"RBF, Matern5/2, Matern3/2, RQ, PiecewisePolynomial\")\n",
    "        \n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(base_kernel,\n",
    "                                                         batch_shape=batch_shape, \n",
    "                                                         ard_num_dims=None)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "\n",
    "\n",
    "class DeepGP2(gpytorch.models.deep_gps.DeepGP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_x,\n",
    "        train_y,\n",
    "        hidden_dim = 4,\n",
    "        inducing_num = 512,\n",
    "        covar_types = [\"RBF\", \"RBF\"],\n",
    "    ):\n",
    "        num_tasks = train_y.size(-1)\n",
    "\n",
    "        layer1 = DGPHiddenLayer(\n",
    "            input_dims=train_x.size(-1),\n",
    "            output_dims=hidden_dim,\n",
    "            num_inducing=inducing_num,\n",
    "            covar_type=covar_types[0],\n",
    "            train_x_for_init=train_x,\n",
    "        )\n",
    "        layer2 = DGPHiddenLayer(\n",
    "            input_dims=hidden_dim,\n",
    "            output_dims=num_tasks,\n",
    "            num_inducing=inducing_num,\n",
    "            covar_type=covar_types[1],\n",
    "            linear_mean=True,\n",
    "            train_x_for_init=train_x,\n",
    "        )\n",
    "\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList([layer1, layer2])\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers[0](x)\n",
    "        return self.layers[1](x)\n",
    "    \n",
    "    def predict(self, test_x):\n",
    "        # with gpytorch.settings.fast_pred_var():\n",
    "        preds = self.likelihood(self(test_x)).to_data_independent_dist()\n",
    "\n",
    "        return preds.mean.mean(0).squeeze(), preds.variance.mean(0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26630edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dgp_minibatch(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    hidden_dim = 4,\n",
    "    inducing_num = 512,\n",
    "    num_iterations = 3000,\n",
    "    patience = 100,\n",
    "    batch_size = 256,\n",
    "    eval_every = 200,\n",
    "    eval_batch_size = 1024,\n",
    "    lr = 0.05,\n",
    "    device = \"cuda\"\n",
    "):\n",
    "    train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "\n",
    "    model = DeepGP2(\n",
    "        train_x, train_y, hidden_dim, inducing_num\n",
    "    ).to(device)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    mll = gpytorch.mlls.DeepApproximateMLL(\n",
    "        gpytorch.mlls.VariationalELBO(\n",
    "            likelihood=model.likelihood, model=model, num_data=train_y.size(0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    best_state = model.state_dict()\n",
    "    no_improve = 0\n",
    "\n",
    "    loader = itertools.cycle(\n",
    "        DataLoader(TensorDataset(train_x, train_y), batch_size, shuffle=True)\n",
    "    )\n",
    "\n",
    "    # --- jitter ---\n",
    "    jitter_ctx = gpytorch.settings.variational_cholesky_jitter(1e-3)\n",
    "\n",
    "    with tqdm.tqdm(total=num_iterations, desc=\"Training DGP\") as pbar, jitter_ctx:\n",
    "        for step in range(num_iterations):\n",
    "            x_batch, y_batch = next(loader)\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step + 1) % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "                    total_loss = 0.0\n",
    "                    for i in range(0, train_x.size(0), eval_batch_size):\n",
    "                        xb, yb = (\n",
    "                            train_x[i : i + eval_batch_size],\n",
    "                            train_y[i : i + eval_batch_size],\n",
    "                        )\n",
    "                        out = model(xb)\n",
    "                        total_loss += -mll(out, yb).item() * yb.size(0)\n",
    "                full_loss = total_loss / train_x.size(0)\n",
    "                pbar.set_postfix(loss=f\"{full_loss:.4f}\")\n",
    "                model.train()\n",
    "\n",
    "                if full_loss < best_loss - 1e-4:\n",
    "                    best_loss, best_state, no_improve = full_loss, model.state_dict(), 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= patience:\n",
    "                        print(\"Early stopping\")\n",
    "                        break\n",
    "            pbar.update(1)\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da94589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DGP:  12%|█▏        | 583/5000 [00:27<03:28, 21.22it/s, loss=10.5873]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dgp_model\u001b[38;5;241m=\u001b[39m \u001b[43mTraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dgp_minibatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGP_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeepGP3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y_21\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                \u001b[49m\u001b[43minducing_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcovar_types\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRBF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                \u001b[49m\u001b[43meval_every\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\programming\\python\\Multioutput_FixedG_GP\\GP_functions\\Training.py:976\u001b[0m, in \u001b[0;36mtrain_dgp_minibatch\u001b[1;34m(DGP_model, train_x, train_y, hidden_dim, inducing_num, covar_types, num_iterations, patience, batch_size, eval_every, eval_batch_size, lr, device)\u001b[0m\n\u001b[0;32m    974\u001b[0m                     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    975\u001b[0m                     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 976\u001b[0m         \u001b[43mpbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    978\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(best_state)\n\u001b[0;32m    979\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\tqdm\\std.py:1242\u001b[0m, in \u001b[0;36mtqdm.update\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dn(dn)\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dt(dt)\n\u001b[1;32m-> 1242\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlock_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_miniters:\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval \u001b[38;5;129;01mand\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\tqdm\\std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m-> 1347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\tqdm\\std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[1;32m-> 1495\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\tqdm\\std.py:459\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[0;32m    458\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[1;32m--> 459\u001b[0m     \u001b[43mfp_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_len\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlen_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\tqdm\\std.py:453\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[0;32m    452\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[1;32m--> 453\u001b[0m     \u001b[43mfp_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\tqdm\\utils.py:196\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\ipykernel\\iostream.py:609\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    610\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dgp_model= Training.train_dgp_minibatch(GP_models.DeepGP3, \n",
    "                                train_x, train_y_21,\n",
    "                                hidden_dim = [10,10],\n",
    "                                inducing_num = 100,\n",
    "                                covar_types = [\"RBF\"]*3, \n",
    "                                num_iterations = 5000,\n",
    "                                patience = 100,\n",
    "                                batch_size = 256,\n",
    "                                eval_every = 100,\n",
    "                                eval_batch_size = 1024,\n",
    "                                lr = 0.05,\n",
    "                                device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a4f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_x.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41796da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgp_model.eval()\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    mean, var = dgp_model.predict(test_x[0,:].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b6fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgp_model.predict(test_x[0,:].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_21[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76b6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
