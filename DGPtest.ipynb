{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265579b4",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f23c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "from linear_operator import settings\n",
    "\n",
    "import pyro\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68cd65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GP_functions.Loss_function as Loss_function\n",
    "import GP_functions.bound as bound\n",
    "import GP_functions.Estimation as Estimation\n",
    "import GP_functions.Training as Training\n",
    "import GP_functions.Prediction as Prediction\n",
    "import GP_functions.GP_models as GP_models\n",
    "import GP_functions.Tools as Tools\n",
    "import GP_functions.FeatureE as FeatureE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdfbef9",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2741d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Data/X_train.csv', header=None, delimiter=',').values\n",
    "X_test = pd.read_csv('Data/X_test.csv', header=None, delimiter=',').values\n",
    "\n",
    "Y_train_21 = pd.read_csv('Data/Y_train_std_21.csv', header=None, delimiter=',').values\n",
    "Y_test_21 = pd.read_csv('Data/Y_test_std_21.csv', header=None, delimiter=',').values\n",
    "\n",
    "Y_train_std = pd.read_csv('Data/Y_train_std.csv', header=None, delimiter=',').values\n",
    "Y_test_std = pd.read_csv('Data/Y_test_std.csv', header=None, delimiter=',').values\n",
    "\n",
    "\n",
    "train_x = torch.tensor(X_train, dtype=torch.float32)\n",
    "test_x = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "train_y_21 = torch.tensor(Y_train_21, dtype=torch.float32)\n",
    "test_y_21 = torch.tensor(Y_test_21, dtype=torch.float32)\n",
    "\n",
    "# train_y = torch.tensor(Y_train_std, dtype=torch.float32)\n",
    "# test_y = torch.tensor(Y_test_std, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe41da",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGPHiddenLayer(gpytorch.models.deep_gps.DeepGPLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims,\n",
    "        output_dims,\n",
    "        num_inducing = 512,\n",
    "        covar_type = \"RBF\",\n",
    "        linear_mean = False,\n",
    "        train_x_for_init = None\n",
    "    ):\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        batch_shape = torch.Size([output_dims])\n",
    "\n",
    "        if train_x_for_init is not None:\n",
    "            idx = torch.randperm(train_x_for_init.size(0))[:num_inducing]\n",
    "            inducing_points = train_x_for_init[idx].clone()\n",
    "            inducing_points = inducing_points.unsqueeze(0).expand(\n",
    "                output_dims, -1, -1\n",
    "            )  # B x M x D\n",
    "        else:\n",
    "            inducing_points = (\n",
    "                torch.rand(output_dims, num_inducing, input_dims) * 4.9 + 0.1\n",
    "            )\n",
    "\n",
    "        variational_dist = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_dist,\n",
    "            learn_inducing_locations=True,\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy, input_dims, output_dims)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.ZeroMean() if linear_mean else gpytorch.means.LinearMean(input_dims)\n",
    "        \n",
    "        if covar_type == 'Matern5/2':\n",
    "            base_kernel = gpytorch.kernels.MaternKernel(nu=2.5,\n",
    "                                                        batch_shape=batch_shape,\n",
    "                                                        ard_num_dims=input_dims)\n",
    "        elif covar_type == 'RBF':\n",
    "            base_kernel = gpytorch.kernels.RBFKernel(batch_shape=batch_shape,\n",
    "                                                     ard_num_dims=input_dims)\n",
    "        elif covar_type == 'Matern3/2':\n",
    "            base_kernel = gpytorch.kernels.MaternKernel(nu=1.5,\n",
    "                                                        batch_shape=batch_shape,\n",
    "                                                        ard_num_dims=input_dims)\n",
    "        elif covar_type == 'RQ':\n",
    "            base_kernel = gpytorch.kernels.RQKernel(batch_shape=batch_shape,\n",
    "                                                    ard_num_dims=input_dims)\n",
    "        elif covar_type == 'PiecewisePolynomial':\n",
    "            base_kernel = gpytorch.kernels.PiecewisePolynomialKernel(q=2,\n",
    "                                                                     batch_shape=batch_shape,\n",
    "                                                                     ard_num_dims=input_dims)\n",
    "        else:\n",
    "            raise ValueError(\"RBF, Matern5/2, Matern3/2, RQ, PiecewisePolynomial\")\n",
    "        \n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(base_kernel,\n",
    "                                                         batch_shape=batch_shape, \n",
    "                                                         ard_num_dims=None)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "\n",
    "\n",
    "class DeepGP2(gpytorch.models.deep_gps.DeepGP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_x,\n",
    "        train_y,\n",
    "        hidden_dim = 4,\n",
    "        inducing_num = 512,\n",
    "        covar_types = [\"RBF\", \"RBF\"],\n",
    "    ):\n",
    "        num_tasks = train_y.size(-1)\n",
    "\n",
    "        layer1 = DGPHiddenLayer(\n",
    "            input_dims=train_x.size(-1),\n",
    "            output_dims=hidden_dim,\n",
    "            num_inducing=inducing_num,\n",
    "            covar_type=covar_types[0],\n",
    "            train_x_for_init=train_x,\n",
    "        )\n",
    "        layer2 = DGPHiddenLayer(\n",
    "            input_dims=hidden_dim,\n",
    "            output_dims=num_tasks,\n",
    "            num_inducing=inducing_num,\n",
    "            covar_type=covar_types[1],\n",
    "            linear_mean=True,\n",
    "            train_x_for_init=train_x,\n",
    "        )\n",
    "\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList([layer1, layer2])\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers[0](x)\n",
    "        return self.layers[1](x)\n",
    "    \n",
    "    def predict(self, test_x):\n",
    "        # with gpytorch.settings.fast_pred_var():\n",
    "        preds = self.likelihood(self(test_x)).to_data_independent_dist()\n",
    "\n",
    "        return preds.mean.mean(0).squeeze(), preds.variance.mean(0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26630edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dgp_minibatch(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    hidden_dim = 4,\n",
    "    inducing_num = 512,\n",
    "    num_iterations = 3000,\n",
    "    patience = 100,\n",
    "    batch_size = 256,\n",
    "    eval_every = 200,\n",
    "    eval_batch_size = 1024,\n",
    "    lr = 0.05,\n",
    "    device = \"cuda\"\n",
    "):\n",
    "    train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "\n",
    "    model = DeepGP2(\n",
    "        train_x, train_y, hidden_dim, inducing_num\n",
    "    ).to(device)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    mll = gpytorch.mlls.DeepApproximateMLL(\n",
    "        gpytorch.mlls.VariationalELBO(\n",
    "            likelihood=model.likelihood, model=model, num_data=train_y.size(0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    best_state = model.state_dict()\n",
    "    no_improve = 0\n",
    "\n",
    "    loader = itertools.cycle(\n",
    "        DataLoader(TensorDataset(train_x, train_y), batch_size, shuffle=True)\n",
    "    )\n",
    "\n",
    "    # --- jitter ---\n",
    "    jitter_ctx = gpytorch.settings.variational_cholesky_jitter(1e-3)\n",
    "\n",
    "    with tqdm.tqdm(total=num_iterations, desc=\"Training DGP\") as pbar, jitter_ctx:\n",
    "        for step in range(num_iterations):\n",
    "            x_batch, y_batch = next(loader)\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step + 1) % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "                    total_loss = 0.0\n",
    "                    for i in range(0, train_x.size(0), eval_batch_size):\n",
    "                        xb, yb = (\n",
    "                            train_x[i : i + eval_batch_size],\n",
    "                            train_y[i : i + eval_batch_size],\n",
    "                        )\n",
    "                        out = model(xb)\n",
    "                        total_loss += -mll(out, yb).item() * yb.size(0)\n",
    "                full_loss = total_loss / train_x.size(0)\n",
    "                pbar.set_postfix(loss=f\"{full_loss:.4f}\")\n",
    "                model.train()\n",
    "\n",
    "                if full_loss < best_loss - 1e-4:\n",
    "                    best_loss, best_state, no_improve = full_loss, model.state_dict(), 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= patience:\n",
    "                        print(\"Early stopping\")\n",
    "                        break\n",
    "            pbar.update(1)\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da94589",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgp_model= Training.train_dgp_minibatch(GP_models.DeepGP2, \n",
    "                                train_x, train_y_21,\n",
    "                                hidden_dim = 10,\n",
    "                                inducing_num = 300,\n",
    "                                covar_types = [\"RQ\", \"RBF\"], \n",
    "                                num_iterations = 5000,\n",
    "                                patience = 100,\n",
    "                                batch_size = 256,\n",
    "                                eval_every = 100,\n",
    "                                eval_batch_size = 1024,\n",
    "                                lr = 0.05,\n",
    "                                device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20632d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model_state_dict': dgp_model.state_dict(),\n",
    "    'model_params': {\n",
    "        'num_hidden_dgp_dims': 10,\n",
    "        'inducing_num': 300,\n",
    "        'covar_types': [\"RQ\", \"RBF\"],\n",
    "        'input_dim': train_x.size(1),\n",
    "        'output_dim': train_y_21.size(1)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "save_path = 'final_dgp_2_checkpoint_21.pth'\n",
    "torch.save(checkpoint, save_path)\n",
    "print(f\"save {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e66af90",
   "metadata": {},
   "source": [
    "# Read Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb750140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultitaskGaussianLikelihood(\n",
       "  (raw_task_noises_constraint): GreaterThan(1.000E-04)\n",
       "  (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_x, train_y_21 = train_x.to(device), train_y_21.to(device)\n",
    "\n",
    "ckpt_path = 'final_dgp_2_checkpoint_21.pth'\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "state_dict   = checkpoint['model_state_dict']\n",
    "model_params = checkpoint['model_params']\n",
    "\n",
    "dgp_model = GP_models.DeepGP2(\n",
    "    train_x, train_y_21, \n",
    "    hidden_dim = model_params['num_hidden_dgp_dims'], \n",
    "    inducing_num = model_params['inducing_num'], \n",
    "    covar_types = model_params['covar_types']\n",
    ").to(device)\n",
    "\n",
    "dgp_model.load_state_dict(state_dict, strict=False)   # strict=False 可防止版本微调导致的非关键键报错\n",
    "dgp_model.eval()\n",
    "dgp_model.likelihood.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67a4f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_x.to('cuda')\n",
    "test_y_21 = test_y_21.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41796da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrad1/2633042r/miniconda3/envs/GPyTorch/lib/python3.8/site-packages/linear_operator/utils/interpolation.py:71: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987301168/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  summing_matrix = cls(summing_matrix_indices, summing_matrix_values, size)\n",
      "/home/pgrad1/2633042r/miniconda3/envs/GPyTorch/lib/python3.8/site-packages/linear_operator/utils/interpolation.py:71: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1704987301168/work/torch/csrc/utils/tensor_new.cpp:618.)\n",
      "  summing_matrix = cls(summing_matrix_indices, summing_matrix_values, size)\n"
     ]
    }
   ],
   "source": [
    "dgp_model.eval()\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    mean, var = dgp_model.predict(test_x[0,:].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c3b6fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.8652,  4.0993, -0.2189,  1.5201,  1.0206,  1.3828, -0.2697,  0.0336,\n",
       "         -0.3237,  0.0910,  0.0044,  0.1388, -0.0638, -0.2080,  0.1221, -0.1245,\n",
       "         -0.0471, -0.0145,  0.0510,  0.0481,  0.0224], device='cuda:0',\n",
       "        grad_fn=<SqueezeBackward0>),\n",
       " tensor([0.0031, 0.0172, 0.0184, 0.0142, 0.0106, 0.0079, 0.0027, 0.0026, 0.0030,\n",
       "         0.0024, 0.0023, 0.0027, 0.0025, 0.0025, 0.0025, 0.0020, 0.0025, 0.0021,\n",
       "         0.0024, 0.0020, 0.0019], device='cuda:0', grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgp_model.predict(test_x[0,:].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96e4afb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9226,  4.2240, -0.2954,  1.5937,  1.0363,  1.3768, -0.2562,  0.0340,\n",
       "        -0.3592,  0.1005,  0.0101,  0.1750, -0.0758, -0.2362,  0.1707, -0.1471,\n",
       "        -0.0780, -0.0288,  0.0638,  0.0508,  0.0267])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_21[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(dgp_model.likelihood(dgp_model(test_x[0,:].unsqueeze(0))).to_data_independent_dist().covariance_matrix).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe601e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgp_model.likelihood(dgp_model(test_x[0,:].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a677cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgp_model.likelihood(dgp_model(test_x[0,:].unsqueeze(0))).to_data_independent_dist().covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65873491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgp_predict(dgpmodel, x, mc=10):\n",
    "    with gpytorch.settings.fast_pred_var(), gpytorch.settings.num_likelihood_samples(mc):\n",
    "        dist = dgpmodel.likelihood(dgpmodel(x)).to_data_independent_dist()\n",
    "    mean = dist.mean.mean(0).squeeze(0)\n",
    "    var  = dist.variance.mean(0).squeeze(0)\n",
    "    return pyro.distributions.Independent(\n",
    "               pyro.distributions.Normal(mean, var.sqrt()), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67ed5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgp_predict_cov(dgpmodel,\n",
    "               x,\n",
    "               num_mc: int = 10,\n",
    "               jitter: float = 1e-6,\n",
    "               device=\"cuda\"):\n",
    "\n",
    "    x = x.to(device)\n",
    "\n",
    "    with gpytorch.settings.fast_pred_var(), gpytorch.settings.num_likelihood_samples(num_mc):\n",
    "        post = dgpmodel.likelihood(dgpmodel(x)).to_data_independent_dist()\n",
    "        # mean : [num_mc, 1, T]\n",
    "        # cov_matrix : [num_mc, 1, T, T]\n",
    "    mean_mc = post.mean.squeeze(1)                     # [num_mc, T]\n",
    "    cov_mc  = post.covariance_matrix.squeeze(1)        # [num_mc, T, T]\n",
    "\n",
    "    # 2. Moment matching across MC dimension\n",
    "    mu_bar = mean_mc.mean(dim=0)                       # [T]\n",
    "    centered = mean_mc - mu_bar                        # [num_mc, T]\n",
    "    cov_mu   = (centered.unsqueeze(2)                  # [num_mc, T, 1]\n",
    "                @ centered.unsqueeze(1))               # [num_mc, 1, T] → [num_mc, T, T]\n",
    "\n",
    "    Sigma_bar = cov_mc.mean(dim=0) + cov_mu.mean(dim=0)  # [T, T]\n",
    "\n",
    "    Sigma_bar = Sigma_bar + jitter * torch.eye(Sigma_bar.size(0),\n",
    "                                               device=Sigma_bar.device)\n",
    "\n",
    "    return pyro.distributions.MultivariateNormal(mu_bar, covariance_matrix=Sigma_bar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379fb003",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgp_predict(dgp_model, test_x[0,:].unsqueeze(0), mc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c68f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = dgp_predict_cov(dgp_model, test_x[0,:].unsqueeze(0), num_mc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.covariance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68c5e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcmc_Uniform_dgp(Pre_function, Models, row_idx, test_y, bounds, num_sampling=2000, warmup_step=1000, num_chains=1, device='cuda'):\n",
    "    test_y = test_y.to(dtype=torch.float32, device=device)\n",
    "    \n",
    "    bounds = [\n",
    "        (\n",
    "            torch.tensor(b[0], dtype=torch.float32, device=device),\n",
    "            torch.tensor(b[1], dtype=torch.float32, device=device)\n",
    "        ) for b in bounds\n",
    "    ]\n",
    "    \n",
    "    def model():\n",
    "        params = []\n",
    "        for i, (min_val, max_val) in enumerate(bounds):\n",
    "            param_i = pyro.sample(f'param_{i}', dist.Uniform(min_val, max_val))\n",
    "            params.append(param_i)\n",
    "        \n",
    "        theta = torch.stack(params)\n",
    "        \n",
    "        gp_pred = Pre_function(Models, theta.unsqueeze(0))\n",
    "        \n",
    "        y_obs = test_y[row_idx, :]\n",
    "        pyro.sample('obs', gp_pred, obs=y_obs)\n",
    "    \n",
    "    nuts_kernel = pyro.infer.mcmc.RandomWalkKernel(model)\n",
    "    mcmc = MCMC(nuts_kernel, num_samples=num_sampling, warmup_steps=warmup_step, num_chains=num_chains)\n",
    "    \n",
    "    mcmc.run()\n",
    "    \n",
    "    return mcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43574a49",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyro.infer.mcmc' has no attribute 'RandomWalkKernel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m local_train_x, local_train_y \u001b[38;5;241m=\u001b[39m Tools\u001b[38;5;241m.\u001b[39mfind_k_nearest_neighbors_GPU(input_point, train_x, train_y_21, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      5\u001b[0m bounds \u001b[38;5;241m=\u001b[39m bound\u001b[38;5;241m.\u001b[39mget_bounds(local_train_x)\n\u001b[0;32m----> 7\u001b[0m mcmc_result_Uniform \u001b[38;5;241m=\u001b[39m \u001b[43mrun_mcmc_Uniform_dgp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdgp_predict_cov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdgp_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mrow_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_y_21\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mnum_sampling\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 24\u001b[0m, in \u001b[0;36mrun_mcmc_Uniform_dgp\u001b[0;34m(Pre_function, Models, row_idx, test_y, bounds, num_sampling, warmup_step, num_chains, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m     y_obs \u001b[38;5;241m=\u001b[39m test_y[row_idx, :]\n\u001b[1;32m     22\u001b[0m     pyro\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m, gp_pred, obs\u001b[38;5;241m=\u001b[39my_obs)\n\u001b[0;32m---> 24\u001b[0m nuts_kernel \u001b[38;5;241m=\u001b[39m \u001b[43mpyro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomWalkKernel\u001b[49m(model)\n\u001b[1;32m     25\u001b[0m mcmc \u001b[38;5;241m=\u001b[39m MCMC(nuts_kernel, num_samples\u001b[38;5;241m=\u001b[39mnum_sampling, warmup_steps\u001b[38;5;241m=\u001b[39mwarmup_step, num_chains\u001b[38;5;241m=\u001b[39mnum_chains)\n\u001b[1;32m     27\u001b[0m mcmc\u001b[38;5;241m.\u001b[39mrun()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyro.infer.mcmc' has no attribute 'RandomWalkKernel'"
     ]
    }
   ],
   "source": [
    "row_idx = 0\n",
    "\n",
    "input_point = test_y_21[row_idx, :]\n",
    "local_train_x, local_train_y = Tools.find_k_nearest_neighbors_GPU(input_point, train_x, train_y_21, k=100)\n",
    "bounds = bound.get_bounds(local_train_x)\n",
    "\n",
    "mcmc_result_Uniform = run_mcmc_Uniform_dgp(dgp_predict_cov, dgp_model, \n",
    "                                           row_idx, test_y_21, bounds, \n",
    "                                           num_sampling = 120, warmup_step = 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a33d881",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m idata \u001b[38;5;241m=\u001b[39m \u001b[43maz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pyro\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmcmc_result_Uniform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m az\u001b[38;5;241m.\u001b[39mplot_trace(idata)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/miniconda3/envs/GPyTorch/lib/python3.8/site-packages/arviz/data/io_pyro.py:329\u001b[0m, in \u001b[0;36mfrom_pyro\u001b[0;34m(posterior, prior, posterior_predictive, log_likelihood, predictions, constant_data, predictions_constant_data, coords, dims, pred_dims, num_chains)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pyro\u001b[39m(\n\u001b[1;32m    285\u001b[0m     posterior\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m     num_chains\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    297\u001b[0m ):\n\u001b[1;32m    298\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert Pyro data into an InferenceData object.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m    For a usage example read the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m        Number of chains used for sampling. Ignored if posterior is present.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPyroConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposterior\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposterior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprior\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposterior_predictive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposterior_predictive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconstant_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstant_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions_constant_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions_constant_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_chains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_inference_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/GPyTorch/lib/python3.8/site-packages/arviz/data/io_pyro.py:272\u001b[0m, in \u001b[0;36mPyroConverter.to_inference_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_inference_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert all available data to an InferenceData object.\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InferenceData(\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m    271\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposterior\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposterior_to_xarray(),\n\u001b[0;32m--> 272\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_stats_to_xarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    273\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_likelihood\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_likelihood_to_xarray(),\n\u001b[1;32m    274\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposterior_predictive\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposterior_predictive_to_xarray(),\n\u001b[1;32m    275\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions_to_xarray(),\n\u001b[1;32m    276\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstant_data_to_xarray(),\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions_constant_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions_constant_data_to_xarray(),\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriors_to_xarray(),\n\u001b[1;32m    279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobserved_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobserved_data_to_xarray(),\n\u001b[1;32m    280\u001b[0m         }\n\u001b[1;32m    281\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/GPyTorch/lib/python3.8/site-packages/arviz/data/base.py:46\u001b[0m, in \u001b[0;36mrequires.__call__.<locals>.wrapped\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m([\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, prop_i) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m prop_i \u001b[38;5;129;01min\u001b[39;00m prop]):\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/GPyTorch/lib/python3.8/site-packages/arviz/data/io_pyro.py:128\u001b[0m, in \u001b[0;36mPyroConverter.sample_stats_to_xarray\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract sample_stats from Pyro posterior.\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m divergences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposterior\u001b[38;5;241m.\u001b[39mdiagnostics()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdivergences\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 128\u001b[0m diverging \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnchains, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndraws), dtype\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28msorted\u001b[39m(divergences)):\n\u001b[1;32m    130\u001b[0m     diverging[i, divergences[k]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/GPyTorch/lib/python3.8/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "idata = az.from_pyro(mcmc_result_Uniform)\n",
    "az.plot_trace(idata)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "summary = az.summary(idata, hdi_prob=0.95)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
