{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265579b4",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f23c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "from linear_operator import settings\n",
    "\n",
    "import pyro\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68cd65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GP_functions.Loss_function as Loss_function\n",
    "import GP_functions.bound as bound\n",
    "import GP_functions.Estimation as Estimation\n",
    "import GP_functions.Training as Training\n",
    "import GP_functions.Prediction as Prediction\n",
    "import GP_functions.GP_models as GP_models\n",
    "import GP_functions.Tools as Tools\n",
    "import GP_functions.FeatureE as FeatureE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdfbef9",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2741d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Data/X_train.csv', header=None, delimiter=',').values\n",
    "X_test = pd.read_csv('Data/X_test.csv', header=None, delimiter=',').values\n",
    "\n",
    "Y_train_21 = pd.read_csv('Data/Y_train_std_21.csv', header=None, delimiter=',').values\n",
    "Y_test_21 = pd.read_csv('Data/Y_test_std_21.csv', header=None, delimiter=',').values\n",
    "\n",
    "Y_train_std = pd.read_csv('Data/Y_train_std.csv', header=None, delimiter=',').values\n",
    "Y_test_std = pd.read_csv('Data/Y_test_std.csv', header=None, delimiter=',').values\n",
    "\n",
    "\n",
    "train_x = torch.tensor(X_train, dtype=torch.float32)\n",
    "test_x = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "train_y_21 = torch.tensor(Y_train_21, dtype=torch.float32)\n",
    "test_y_21 = torch.tensor(Y_test_21, dtype=torch.float32)\n",
    "\n",
    "# train_y = torch.tensor(Y_train_std, dtype=torch.float32)\n",
    "# test_y = torch.tensor(Y_test_std, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe41da",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b58e60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGPHiddenLayer(gpytorch.models.deep_gps.DeepGPLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims,\n",
    "        output_dims,\n",
    "        num_inducing = 512,\n",
    "        covar_type = \"RBF\",\n",
    "        linear_mean = False,\n",
    "        train_x_for_init = None\n",
    "    ):\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        batch_shape = torch.Size([output_dims])\n",
    "\n",
    "        if train_x_for_init is not None:\n",
    "            idx = torch.randperm(train_x_for_init.size(0))[:num_inducing]\n",
    "            inducing_points = train_x_for_init[idx].clone()\n",
    "            inducing_points = inducing_points.unsqueeze(0).expand(\n",
    "                output_dims, -1, -1\n",
    "            )  # B x M x D\n",
    "        else:\n",
    "            inducing_points = (\n",
    "                torch.rand(output_dims, num_inducing, input_dims) * 4.9 + 0.1\n",
    "            )\n",
    "\n",
    "        variational_dist = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_dist,\n",
    "            learn_inducing_locations=True,\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy, input_dims, output_dims)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.ZeroMean() if linear_mean else gpytorch.means.LinearMean(input_dims)\n",
    "        \n",
    "        if covar_type == 'Matern5/2':\n",
    "            base_kernel = gpytorch.kernels.MaternKernel(nu=2.5,\n",
    "                                                        batch_shape=batch_shape,\n",
    "                                                        ard_num_dims=input_dims)\n",
    "        elif covar_type == 'RBF':\n",
    "            base_kernel = gpytorch.kernels.RBFKernel(batch_shape=batch_shape,\n",
    "                                                     ard_num_dims=input_dims)\n",
    "        elif covar_type == 'Matern3/2':\n",
    "            base_kernel = gpytorch.kernels.MaternKernel(nu=1.5,\n",
    "                                                        batch_shape=batch_shape,\n",
    "                                                        ard_num_dims=input_dims)\n",
    "        elif covar_type == 'RQ':\n",
    "            base_kernel = gpytorch.kernels.RQKernel(batch_shape=batch_shape,\n",
    "                                                    ard_num_dims=input_dims)\n",
    "        elif covar_type == 'PiecewisePolynomial':\n",
    "            base_kernel = gpytorch.kernels.PiecewisePolynomialKernel(q=2,\n",
    "                                                                     batch_shape=batch_shape,\n",
    "                                                                     ard_num_dims=input_dims)\n",
    "        else:\n",
    "            raise ValueError(\"RBF, Matern5/2, Matern3/2, RQ, PiecewisePolynomial\")\n",
    "        \n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(base_kernel,\n",
    "                                                         batch_shape=batch_shape, \n",
    "                                                         ard_num_dims=None)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "\n",
    "\n",
    "class DeepGP2(gpytorch.models.deep_gps.DeepGP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_x,\n",
    "        train_y,\n",
    "        hidden_dim = 4,\n",
    "        inducing_num = 512,\n",
    "        covar_types = [\"RBF\", \"RBF\"],\n",
    "    ):\n",
    "        num_tasks = train_y.size(-1)\n",
    "\n",
    "        layer1 = DGPHiddenLayer(\n",
    "            input_dims=train_x.size(-1),\n",
    "            output_dims=hidden_dim,\n",
    "            num_inducing=inducing_num,\n",
    "            covar_type=covar_types[0],\n",
    "            train_x_for_init=train_x,\n",
    "        )\n",
    "        layer2 = DGPHiddenLayer(\n",
    "            input_dims=hidden_dim,\n",
    "            output_dims=num_tasks,\n",
    "            num_inducing=inducing_num,\n",
    "            covar_type=covar_types[1],\n",
    "            linear_mean=True,\n",
    "            train_x_for_init=train_x,\n",
    "        )\n",
    "\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList([layer1, layer2])\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers[0](x)\n",
    "        return self.layers[1](x)\n",
    "    \n",
    "    def predict(self, test_x):\n",
    "        # with gpytorch.settings.fast_pred_var():\n",
    "        preds = self.likelihood(self(test_x)).to_data_independent_dist()\n",
    "\n",
    "        return preds.mean.mean(0).squeeze(), preds.variance.mean(0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26630edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dgp_minibatch(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    hidden_dim = 4,\n",
    "    inducing_num = 512,\n",
    "    num_iterations = 3000,\n",
    "    patience = 100,\n",
    "    batch_size = 256,\n",
    "    eval_every = 200,\n",
    "    eval_batch_size = 1024,\n",
    "    lr = 0.05,\n",
    "    device = \"cuda\"\n",
    "):\n",
    "    train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "\n",
    "    model = DeepGP2(\n",
    "        train_x, train_y, hidden_dim, inducing_num\n",
    "    ).to(device)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    mll = gpytorch.mlls.DeepApproximateMLL(\n",
    "        gpytorch.mlls.VariationalELBO(\n",
    "            likelihood=model.likelihood, model=model, num_data=train_y.size(0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    best_state = model.state_dict()\n",
    "    no_improve = 0\n",
    "\n",
    "    loader = itertools.cycle(\n",
    "        DataLoader(TensorDataset(train_x, train_y), batch_size, shuffle=True)\n",
    "    )\n",
    "\n",
    "    # --- jitter ---\n",
    "    jitter_ctx = gpytorch.settings.variational_cholesky_jitter(1e-3)\n",
    "\n",
    "    with tqdm.tqdm(total=num_iterations, desc=\"Training DGP\") as pbar, jitter_ctx:\n",
    "        for step in range(num_iterations):\n",
    "            x_batch, y_batch = next(loader)\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step + 1) % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "                    total_loss = 0.0\n",
    "                    for i in range(0, train_x.size(0), eval_batch_size):\n",
    "                        xb, yb = (\n",
    "                            train_x[i : i + eval_batch_size],\n",
    "                            train_y[i : i + eval_batch_size],\n",
    "                        )\n",
    "                        out = model(xb)\n",
    "                        total_loss += -mll(out, yb).item() * yb.size(0)\n",
    "                full_loss = total_loss / train_x.size(0)\n",
    "                pbar.set_postfix(loss=f\"{full_loss:.4f}\")\n",
    "                model.train()\n",
    "\n",
    "                if full_loss < best_loss - 1e-4:\n",
    "                    best_loss, best_state, no_improve = full_loss, model.state_dict(), 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= patience:\n",
    "                        print(\"Early stopping\")\n",
    "                        break\n",
    "            pbar.update(1)\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1da94589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DGP: 100%|██████████| 5000/5000 [04:00<00:00, 20.75it/s, loss=-24.7777]\n"
     ]
    }
   ],
   "source": [
    "dgp_model= train_dgp_minibatch(train_x, train_y_21,\n",
    "                                hidden_dim = 10,\n",
    "                                inducing_num = 100,\n",
    "                                num_iterations = 5000,\n",
    "                                patience = 100,\n",
    "                                batch_size = 256,\n",
    "                                eval_every = 100,\n",
    "                                eval_batch_size = 1024,\n",
    "                                lr = 0.05,\n",
    "                                device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67a4f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_x.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41796da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgp_model.eval()\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    mean, var = dgp_model.predict(test_x[0,:].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c3b6fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.9150,  4.2057, -0.1588,  1.6351,  1.0829,  1.3062, -0.2847, -0.0042,\n",
       "         -0.3159,  0.0779, -0.0245,  0.1177, -0.0522, -0.2008,  0.1500, -0.1301,\n",
       "         -0.0589, -0.0170,  0.0771,  0.0000,  0.0121], device='cuda:0',\n",
       "        grad_fn=<SqueezeBackward0>),\n",
       " tensor([0.0038, 0.0225, 0.0246, 0.0209, 0.0151, 0.0133, 0.0033, 0.0035, 0.0034,\n",
       "         0.0031, 0.0028, 0.0030, 0.0030, 0.0031, 0.0026, 0.0025, 0.0024, 0.0025,\n",
       "         0.0024, 0.0097, 0.0024], device='cuda:0', grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgp_model.predict(test_x[0,:].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96e4afb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9226,  4.2240, -0.2954,  1.5937,  1.0363,  1.3768, -0.2562,  0.0340,\n",
       "        -0.3592,  0.1005,  0.0101,  0.1750, -0.0758, -0.2362,  0.1707, -0.1471,\n",
       "        -0.0780, -0.0288,  0.0638,  0.0508,  0.0267])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_21[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FGPyT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
