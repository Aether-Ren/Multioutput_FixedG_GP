{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265579b4",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f23c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "from linear_operator import settings\n",
    "\n",
    "import pyro\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68cd65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GP_functions.Loss_function as Loss_function\n",
    "import GP_functions.bound as bound\n",
    "import GP_functions.Estimation as Estimation\n",
    "import GP_functions.Training as Training\n",
    "import GP_functions.Prediction as Prediction\n",
    "import GP_functions.GP_models as GP_models\n",
    "import GP_functions.Tools as Tools\n",
    "import GP_functions.FeatureE as FeatureE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdfbef9",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2741d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Data/X_train.csv', header=None, delimiter=',').values\n",
    "X_test = pd.read_csv('Data/X_test.csv', header=None, delimiter=',').values\n",
    "\n",
    "Y_train_21 = pd.read_csv('Data/Y_train_std_21.csv', header=None, delimiter=',').values\n",
    "Y_test_21 = pd.read_csv('Data/Y_test_std_21.csv', header=None, delimiter=',').values\n",
    "\n",
    "Y_train_std = pd.read_csv('Data/Y_train_std.csv', header=None, delimiter=',').values\n",
    "Y_test_std = pd.read_csv('Data/Y_test_std.csv', header=None, delimiter=',').values\n",
    "\n",
    "\n",
    "train_x = torch.tensor(X_train, dtype=torch.float32)\n",
    "test_x = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "train_y_21 = torch.tensor(Y_train_21, dtype=torch.float32)\n",
    "test_y_21 = torch.tensor(Y_test_21, dtype=torch.float32)\n",
    "\n",
    "# train_y = torch.tensor(Y_train_std, dtype=torch.float32)\n",
    "# test_y = torch.tensor(Y_test_std, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe41da",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGPHiddenLayer(gpytorch.models.deep_gps.DeepGPLayer):\n",
    "    def __init__(self, input_dims, output_dims, num_inducing=500, linear_mean=True,\n",
    "                 covar_type='RBF'):\n",
    "\n",
    "        inducing_points = torch.rand(output_dims, num_inducing, input_dims) * (5 - 0.1) + 0.1\n",
    "        # inducing_points = train_x[:num_inducing].unsqueeze(0).expand(output_dims, -1, -1).contiguous()\n",
    "\n",
    "        batch_shape = torch.Size([output_dims])\n",
    "        \n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_shape=batch_shape\n",
    "        )\n",
    "        \n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "        \n",
    "        super().__init__(variational_strategy, input_dims, output_dims)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.ZeroMean() if linear_mean else gpytorch.means.LinearMean(input_dims)\n",
    "        \n",
    "        # 根据 covar_type 选择对应的核函数\n",
    "        if covar_type == 'Matern5/2':\n",
    "            base_kernel = gpytorch.kernels.MaternKernel(nu=2.5,\n",
    "                                                        batch_shape=batch_shape,\n",
    "                                                        ard_num_dims=input_dims)\n",
    "        elif covar_type == 'RBF':\n",
    "            base_kernel = gpytorch.kernels.RBFKernel(batch_shape=batch_shape,\n",
    "                                                     ard_num_dims=input_dims)\n",
    "        elif covar_type == 'Matern3/2':\n",
    "            base_kernel = gpytorch.kernels.MaternKernel(nu=1.5,\n",
    "                                                        batch_shape=batch_shape,\n",
    "                                                        ard_num_dims=input_dims)\n",
    "        elif covar_type == 'RQ':\n",
    "            base_kernel = gpytorch.kernels.RQKernel(batch_shape=batch_shape,\n",
    "                                                    ard_num_dims=input_dims)\n",
    "        elif covar_type == 'PiecewisePolynomial':\n",
    "            base_kernel = gpytorch.kernels.PiecewisePolynomialKernel(q=2,\n",
    "                                                                     batch_shape=batch_shape,\n",
    "                                                                     ard_num_dims=input_dims)\n",
    "        else:\n",
    "            raise ValueError(\"RBF, Matern5/2, Matern3/2, RQ, PiecewisePolynomial\")\n",
    "        \n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(base_kernel,\n",
    "                                                         batch_shape=batch_shape, \n",
    "                                                         ard_num_dims=None)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "\n",
    "\n",
    "class DeepGP_2(gpytorch.models.deep_gps.DeepGP):\n",
    "    def __init__(self, train_x_shape, train_y, num_hidden_dgp_dims = 4, inducing_num = 500, covar_types = ['RBF','Matern3/2']):\n",
    "        num_tasks = train_y.size(-1)\n",
    " \n",
    "        hidden_layer_1 = DGPHiddenLayer(\n",
    "            input_dims=train_x_shape[-1],\n",
    "            output_dims=num_hidden_dgp_dims,\n",
    "            num_inducing=inducing_num, \n",
    "            linear_mean=True,\n",
    "            covar_type=covar_types[0]\n",
    "        )\n",
    "\n",
    "\n",
    "        last_layer = DGPHiddenLayer(\n",
    "            input_dims=hidden_layer_1.output_dims,\n",
    "            output_dims = num_tasks,\n",
    "            num_inducing=inducing_num, \n",
    "            linear_mean=False,\n",
    "            covar_type=covar_types[1]\n",
    "        )\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_layer_1 = hidden_layer_1\n",
    "        self.last_layer = last_layer\n",
    "\n",
    "        # We're going to use a ultitask likelihood instead of the standard GaussianLikelihood\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hidden_rep1 = self.hidden_layer_1(inputs)\n",
    "        output = self.last_layer(hidden_rep1)\n",
    "        return output\n",
    "    \n",
    "    def predict(self, test_x):\n",
    "        # with torch.no_grad():\n",
    "        preds = self.likelihood(self(test_x)).to_data_independent_dist()\n",
    "\n",
    "        return preds.mean.mean(0).squeeze(), preds.variance.mean(0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26630edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DGP_minibatch(\n",
    "    full_train_x, \n",
    "    full_train_y, \n",
    "    DGP_model,\n",
    "    num_hidden_dgp_dims=4, \n",
    "    inducing_num=500, \n",
    "    num_iterations=2000, \n",
    "    patience=50, \n",
    "    device='cuda',\n",
    "    batch_size=32,\n",
    "    eval_every=100,\n",
    "    eval_batch_size=1024,\n",
    "    lr_variational = 0.1,\n",
    "    lr=0.05\n",
    "):\n",
    "    \"\"\"\n",
    "    训练Deep GP (2层) 的完整流程，支持小批量训练、早停、全数据集评估和学习率调度。\n",
    "    \n",
    "    参数说明：\n",
    "    - full_train_x, full_train_y: 训练数据\n",
    "    - num_hidden_dgp_dims: Deep GP中隐藏层维度\n",
    "    - inducing_num: 每层诱导点数量\n",
    "    - num_iterations: 总迭代次数上限\n",
    "    - patience: 早停耐心值 (评估损失连续多少次不下降就停止)\n",
    "    - device: 'cpu' 或 'cuda'\n",
    "    - batch_size: 小批量训练时的批量大小\n",
    "    - eval_every: 每隔多少次迭代进行一次全数据评估\n",
    "    - eval_batch_size: 进行全数据评估时的批量大小\n",
    "    - lr: 初始学习率\n",
    "    \"\"\"\n",
    "\n",
    "    full_train_x = full_train_x.to(device)\n",
    "    full_train_y = full_train_y.to(device)\n",
    "\n",
    "\n",
    "    model = DGP_model(\n",
    "        full_train_x.shape, \n",
    "        full_train_y, \n",
    "        num_hidden_dgp_dims, \n",
    "        inducing_num\n",
    "    ).to(device)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(\n",
    "        model.variational_parameters(),\n",
    "        num_data=full_train_y.size(0),\n",
    "        lr=lr_variational\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mll = gpytorch.mlls.DeepApproximateMLL(\n",
    "        gpytorch.mlls.VariationalELBO(\n",
    "            model.likelihood,\n",
    "            model,\n",
    "            num_data=full_train_y.size(0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_state = model.state_dict()\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        TensorDataset(full_train_x, full_train_y),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    minibatch_iter = itertools.cycle(data_loader)\n",
    "\n",
    "\n",
    "    with tqdm.tqdm(total=num_iterations, desc=\"Training DGP\") as pbar:\n",
    "        for step in range(num_iterations):\n",
    "            x_batch, y_batch = next(minibatch_iter)\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step + 1) % eval_every == 0 or (step == num_iterations - 1):\n",
    "                current_loss = Training.evaluate_full_dataset_loss_dgp(\n",
    "                    model=model,\n",
    "                    x_data=full_train_x,\n",
    "                    y_data=full_train_y,\n",
    "                    mll=mll,\n",
    "                    device=device,\n",
    "                    batch_size=eval_batch_size\n",
    "                )\n",
    "                pbar.set_postfix(full_loss=current_loss)\n",
    "                \n",
    "\n",
    "                if current_loss < best_loss:\n",
    "                    best_loss = current_loss\n",
    "                    best_state = model.state_dict()\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    counter += 1\n",
    "                    if counter >= patience:\n",
    "                        model.load_state_dict(best_state)\n",
    "                        pbar.update(num_iterations - step - 1)\n",
    "                        break\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da94589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DGP:   1%|          | 90/10000 [00:09<17:38,  9.37it/s]\n"
     ]
    },
    {
     "ename": "NotPSDError",
     "evalue": "Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotPSDError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dgp_model\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_DGP_minibatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y_21\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mDeepGP_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hidden_dgp_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minducing_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mlr_variational\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 79\u001b[0m, in \u001b[0;36mtrain_DGP_minibatch\u001b[1;34m(full_train_x, full_train_y, DGP_model, num_hidden_dgp_dims, inducing_num, num_iterations, patience, device, batch_size, eval_every, eval_batch_size, lr_variational, lr)\u001b[0m\n\u001b[0;32m     76\u001b[0m x_batch, y_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     78\u001b[0m variational_ngd_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 79\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, y_batch)\n\u001b[0;32m     81\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "Cell \u001b[1;32mIn[16], line 89\u001b[0m, in \u001b[0;36mDeepGP_2.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m---> 89\u001b[0m     hidden_rep1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layer_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer(hidden_rep1)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\gpytorch\\models\\deep_gps\\deep_gp.py:100\u001b[0m, in \u001b[0;36mDeepGPLayer.__call__\u001b[1;34m(self, inputs, are_samples, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m*\u001b[39minputs\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dims, \u001b[38;5;241m*\u001b[39minputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Now run samples through the GP\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m output \u001b[38;5;241m=\u001b[39m ApproximateGP\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     mean \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\gpytorch\\models\\approximate_gp.py:114\u001b[0m, in \u001b[0;36mApproximateGP.__call__\u001b[1;34m(self, inputs, prior, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    113\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariational_strategy(inputs, prior\u001b[38;5;241m=\u001b[39mprior, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\gpytorch\\variational\\variational_strategy.py:272\u001b[0m, in \u001b[0;36mVariationalStrategy.__call__\u001b[1;34m(self, x, prior, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;66;03m# Mark that we have updated the variational strategy\u001b[39;00m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdated_strategy\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x, prior\u001b[38;5;241m=\u001b[39mprior, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\gpytorch\\variational\\_variational_strategy.py:343\u001b[0m, in \u001b[0;36m_VariationalStrategy.__call__\u001b[1;34m(self, x, prior, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m     x, inducing_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs(x, inducing_points)\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# Get p(u)/q(u)\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m variational_dist_u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariational_distribution\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# Get q(f)\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(variational_dist_u, MultivariateNormal):\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\gpytorch\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\gpytorch\\variational\\_variational_strategy.py:124\u001b[0m, in \u001b[0;36m_VariationalStrategy.variational_distribution\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;129m@cached\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariational_distribution_memo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvariational_distribution\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Distribution:\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variational_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\gpytorch\\variational\\_variational_distribution.py:59\u001b[0m, in \u001b[0;36m_VariationalDistribution.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Distribution:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\gpytorch\\variational\\natural_variational_distribution.py:61\u001b[0m, in \u001b[0;36mNaturalVariationalDistribution.forward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 61\u001b[0m     mean, chol_covar \u001b[38;5;241m=\u001b[39m \u001b[43m_NaturalToMuVarSqrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnatural_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnatural_mat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     res \u001b[38;5;241m=\u001b[39m MultivariateNormal(mean, CholLinearOperator(TriangularLinearOperator(chol_covar)))\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\torch\\autograd\\function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\gpytorch\\variational\\natural_variational_distribution.py:118\u001b[0m, in \u001b[0;36m_NaturalToMuVarSqrt.forward\u001b[1;34m(ctx, nat_mean, nat_covar)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, nat_mean, nat_covar):\n\u001b[1;32m--> 118\u001b[0m     mu, L \u001b[38;5;241m=\u001b[39m \u001b[43m_NaturalToMuVarSqrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnat_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnat_covar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(mu, L)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mu, L\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\gpytorch\\variational\\natural_variational_distribution.py:108\u001b[0m, in \u001b[0;36m_NaturalToMuVarSqrt._forward\u001b[1;34m(nat_mean, nat_covar)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-negative-definite natural covariance. You probably \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdated it using an optimizer other than gpytorch.optim.NGD (such as Adam). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m         )\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    109\u001b[0m L \u001b[38;5;241m=\u001b[39m _triangular_inverse(L_inv, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    110\u001b[0m S \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m@\u001b[39m L\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\gpytorch\\variational\\natural_variational_distribution.py:99\u001b[0m, in \u001b[0;36m_NaturalToMuVarSqrt._forward\u001b[1;34m(nat_mean, nat_covar)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(nat_mean, nat_covar):\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m         L_inv \u001b[38;5;241m=\u001b[39m \u001b[43mpsd_safe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnat_covar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcholesky\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\linear_operator\\utils\\cholesky.py:65\u001b[0m, in \u001b[0;36mpsd_safe_cholesky\u001b[1;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpsd_safe_cholesky\u001b[39m(A, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, jitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_tries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m        :attr:`A` (Tensor):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m            Number of attempts (with successively increasing jitter) to make before raising an error.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[43m_psd_safe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\FGPyT\\lib\\site-packages\\linear_operator\\utils\\cholesky.py:47\u001b[0m, in \u001b[0;36m_psd_safe_cholesky\u001b[1;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(info):\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m L\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NotPSDError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatrix not positive definite after repeatedly adding jitter up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjitter_new\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotPSDError\u001b[0m: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04."
     ]
    }
   ],
   "source": [
    "dgp_model= train_DGP_minibatch(train_x, train_y_21, \n",
    "                               DeepGP_2, num_hidden_dgp_dims=10, inducing_num=100, \n",
    "                               num_iterations=10000, patience=50, \n",
    "                               device='cuda', batch_size=512, eval_every=100, eval_batch_size=1024, \n",
    "                               lr_variational = 0.1,lr=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FGPyT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
